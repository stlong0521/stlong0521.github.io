<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Tianlong's Blog <small>Tianlong Song</small></title>
    <meta name="description" content="">
    <meta name="author" content="Tianlong Song">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://stlong0521.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://stlong0521.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/local.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->
        <link href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate" title="Tianlong's Blog" type="application/atom+xml">
        <link href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate" title="Tianlong's Blog" type="application/rss+xml">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://stlong0521.github.io">Tianlong's Blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about.html">About</a></li>
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


<!--     <div class='article'>
        <div class="content-title">
            <a href="https://stlong0521.github.io/20160403 - NN and DL.html"><h1>Neural Networks and Deep Learning</h1></a>
Sun 03 Apr 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  
        </div>
        
        <div><p>It has been a long time since the idea of neural networks was proposed, but it is really during the last few years that neural networks have become widely used. One of the major enablers is the infrastructure with high computational capability (e.g., cloud computing), which makes the training of large and deep (multilayer) neural networks possible. This post is in no way an exhaustive review of neural networks or deep learning, but rather an entry-level introduction excerpted from a very popular book<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup>.</p>
<h3>Neural Network (NN) Basics</h3>
<p>Let us start with sigmoid neurons, and then find out how they are used in NNs.</p>
<h4>Sigmoid Neurons</h4>
<p>As the smallest unit in NNs, a sigmoid neuron mimics the behaviour of a real neuron in human neural systems. It takes multiple inputs and generates one single output, as in a process of local or partial decision making. More specifically, given a series of inputs <span class="math">\([x_1,x_2,...]\)</span>, a neuron applies the sigmoid function to the weighted sum of the inputs plus a bias, i.e., the output of the neuron is computed as
</p>
<div class="math">\begin{equation}
\sigma(z)=\sigma(\sum_j{w_jx_j}+b)=\frac{1}{1+\exp(-\sum_j{w_jx_j}-b)},
\end{equation}</div>
<p>
where <span class="math">\(z=\sum_j{w_jx_j}+b\)</span> is the weighted input to the neuron, and the sigmoid function, <span class="math">\(\sigma(z)=\frac{1}{1+\exp(-z)}\)</span>, is to approximate the step function as usually used in binary decision making. A natural question is: why do not we just use the step function? The answer is that the step function is not smooth (not differentiable at origin), which disables the gradient method in model learning. With the smoothed version of the step function, we are safe to relate the change at the output to the weight/bias changes by
</p>
<div class="math">\begin{equation}
\Delta{output}=\sum_j{\frac{\partial{output}}{\partial{w_j}}\Delta{w_j}}+\frac{\partial{output}}{\partial{b}}\Delta{b}.
\end{equation}</div>
<h4>The Architecture of NNs</h4>
<p>The architecture of a typical NN is depicted in Fig. 1. As shown, the leftmost layer in this network is called the input layer, and the neurons within this layer are called input neurons. The rightmost or output layer contains the output neuron(s). The middle layer is called a hidden layer, since the neurons in this layer are neither inputs nor outputs.</p>
<figure align="center">
<img src="/figures/20160403/NN.png" alt="Neural Network Architecture">
<figcaption align="center">Fig. 1: The neural network architecture.</figcaption>
</figure>

<p>It was proved that NNs with a single hidden layer can be used to approximate any continuous function to any desired precision. Click <a href="http://neuralnetworksanddeeplearning.com/chap4.html">here</a> to see how. It can be expected that a NN with more neurons/layers could be more accurate in the approximation.</p>
<h4>Learning with Gradient Descent</h4>
<p>Before training a model, we first need to find a way to quantify how well we are achieving. That is, we need to introduce a cost function. Although there are many cost functions available, we will start with the quadratic cost function, which is defined as
</p>
<div class="math">\begin{equation}
C(w,b)=\frac{1}{2n}\sum_x{||y(x)-a||^2},
\end{equation}</div>
<p>
where <span class="math">\(w\)</span> denotes the collection of all weights in the network, <span class="math">\(b\)</span> all the biases, <span class="math">\(n\)</span> is the total number of training inputs, <span class="math">\(a\)</span> is the vector of outputs from the network when <span class="math">\(x\)</span> is input, and the sum is over all training inputs, <span class="math">\(x\)</span>.</p>
<p>Applying the gradient descent method, we can learn the weights and biases by
</p>
<div class="math">\begin{equation}
w_k'=w_k-\eta\frac{\partial{C}}{\partial{w_k}},
\end{equation}</div>
<div class="math">\begin{equation}
b_l'=b_l-\eta\frac{\partial{C}}{\partial{b_l}}.
\end{equation}</div>
<p>
To compute each gradient, we need to take into account all the training input <span class="math">\(x\)</span> in each iteration. However, this slows the learning down if the training data size is large. An idea called <em>stochastic gradient descent (a.k.a., mini-batch learning)</em> can be used to speed up learning. The idea is to estimate the gradient by computing it for a small sample of randomly chosen training inputs.</p>
<h3>The Backpropagation Algorithm: How to Compute Gradients of the Cost Function?</h3>
<p>So far we have known that the model parameters can be learned by the gradient descent method, but the computation of the gradients can be challenging by itself. Note that the network size and the data size can both be very large. In this section, we will see how the backpropagation algorithm helps compute the gradients efficiently.</p>
<h4>Matrix Notation for NNs</h4>
<p>For ease of presentation, let us define some notations first.</p>
<blockquote>
<p><span class="math">\(w_{jk}^l\)</span>: weight from the <span class="math">\(k\)</span>th neuron in layer <span class="math">\(l-1\)</span> to the <span class="math">\(j\)</span>th neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(w^l=\{w_{jk}^l\}\)</span>: matrix including all weights from each neuron in layer <span class="math">\(l-1\)</span> to each neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(b_j^l\)</span>: bias for the <span class="math">\(j\)</span>th neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(b^l=\{b_j^l\}\)</span>: column vector including all biases for each neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(a_j^l=\sigma(\sum_k{w_{jk}^la_k^{l-1}+b_j^l})\)</span>: activation of the <span class="math">\(j\)</span>th neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(a^l=\{a_j^l\}=\sigma(w^la^{l-1}+b^l)\)</span>: column vector including all activations of each neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(z_j^l=\sum_k{w_{jk}^la_k^{l-1}+b_j^l}\)</span>: weighted input to the <span class="math">\(j\)</span>th neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(z^l=\{z_j^l\}=w^la^{l-1}+b^l\)</span>: column vector including all weighted inputs to each neuron in layer <span class="math">\(l\)</span>;</p>
<p><span class="math">\(\delta_j^l=\frac{\partial{C}}{\partial{z_j^l}}\)</span>: gradient of the cost function w.r.t. the weighted input to the <span class="math">\(j\)</span>th neuron in layer <span class="math">\(l\)</span>, <span class="math">\(z_j^l\)</span>;</p>
<p><span class="math">\(\delta^l=\{\delta_j^l\}\)</span>: column vector including all gradients of the cost function w.r.t. the weighted input to each neuron in layer <span class="math">\(l\)</span>.</p>
</blockquote>
<h4>Four Fundamental Equations behind Backpropagation</h4>
<p>There are four fundamental equations behind backpropagation, which will be explained one by one as below.</p>
<p><em>First</em>, the gradient of the cost function w.r.t. the weighted input to each neuron in output layer <span class="math">\(L\)</span> can be computed as
</p>
<div class="math">\begin{equation}
\delta^L=\nabla_{a^L}C \odot \sigma'(z^L),
\end{equation}</div>
<p>
where <span class="math">\(\nabla_{a^L}C=\{\frac{\partial{C}}{\partial{a_j^L}}\}\)</span> is defined to be a column vector whose components are the partial derivatives <span class="math">\(\frac{\partial{C}}{\partial{a_j^L}}\)</span>, <span class="math">\(\sigma'(z)\)</span> is the first-order derivative of the sigmoid function <span class="math">\(\sigma(z)\)</span>, and <span class="math">\(\odot\)</span> represents an element-wise product.</p>
<p><em>Second</em>, the gradient of the cost function w.r.t. the weighted input to each neuron in layer <span class="math">\(l(l&lt;L)\)</span> can be computed from the results of layer <span class="math">\(l+1\)</span> (backpropagation), i.e.,
</p>
<div class="math">\begin{equation}
\delta^l=((w^{l+1})^T\delta^{l+1}) \odot \sigma'(z^l).
\end{equation}</div>
<p><em>Third</em>, the gradient of the cost function w.r.t. each bias can be computed as
</p>
<div class="math">\begin{equation}
\frac{\partial{C}}{\partial{b_j^l}}=\delta_j^l.
\end{equation}</div>
<p><em>Fourth</em>, the gradient of the cost function w.r.t. each weight can be computed as
</p>
<div class="math">\begin{equation}
\frac{\partial{C}}{\partial{w_{jk}^l}}=\delta_j^la_k^{l-1}.
\end{equation}</div>
<p>The four equations above are not straightforward at first sight, but they are all consequences of the chain rule from multivariable calculus. The proof can be found <a href="http://neuralnetworksanddeeplearning.com/chap2.html#proof_of_the_four_fundamental_equations_(optional)">here</a>.</p>
<h4>The Backpropagation Algorithm</h4>
<p>The backpropagation algorithm essentially includes a feedforward process and a backpropagation process. More specifically, in each iteration:</p>
<ol>
<li>Input a mini-batch of <span class="math">\(m\)</span> training examples;</li>
<li>For each training example <span class="math">\(x\)</span>:<ul>
<li>Initialization: set the activations of the input layer by <span class="math">\(a^{x,1}=x\)</span>;</li>
<li>Feedforward: for each <span class="math">\(l=2,3,...,L\)</span>, compute <span class="math">\(z^{x,l}=w^la^{x,l-1}+b^l\)</span> and <span class="math">\(a^{x,l}=\sigma(z^{x,l})\)</span>;</li>
<li>Output error: compute the error vector <span class="math">\(\delta^{x,L}=\nabla_{a^L}C_x \odot \sigma'(z^{x,L})\)</span>;</li>
<li>Backpropagate the error: for <span class="math">\(l=L-1,L-2,...,2\)</span>, compute <span class="math">\(\delta^{x,l}=((w^{l+1})^T\delta^{x,l+1}) \odot \sigma'(z^{x,l})\)</span>.</li>
</ul>
</li>
<li>Compute gradients and apply the gradient descent method by
<div class="math">\begin{equation}
\frac{\partial{C_x}}{\partial{w_{jk}^l}}=\delta_j^{x,l}a_k^{x,l-1},~~\frac{\partial{C_x}}{\partial{b_j^l}}=\delta_j^{x,l},~~w^l=w^l-\frac{\eta}{m}\sum_x{\delta^{x,l}(a^{x,l-1})^T},~~b^l=b^l-\frac{\eta}{m}\sum_x{\delta^{x,l}}.
\end{equation}</div>
</li>
</ol>
<h3>Deep Learning</h3>
<p>Everything seems going well so far! What if our NNs are deep, i.e., with a lot of hidden layers? Typically we expect a deep NN could deliver better performance than shallow ones. Unfortunately it was observed that: for deep NNs, the learning speeds of the first few layers of neurons can be much higher/lower than those of the last few layers, in which case the NNs cannot be well trained. Click <a href="http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem">here</a> to see why. This is known as <em>vanishing/exploding gradient problem</em>. To resolve this problem, it is suggested that we should resort to convolutional networks.</p>
<h4>Convolutional Networks</h4>
<p>Convolutional neural networks use three basic ideas: local receptive fields, shared weights, and pooling. A typical convolutional network is depicted in Fig. 2, where the network includes one input layer, one convolutional layer, one pooling layer and one output layer (fully connected). We will try to understand the three ideas using this network as an example.</p>
<figure align="center">
<img src="/figures/20160403/ConvN.png" alt="Convolutional Networks">
<figcaption align="center">Fig. 2: The convolutional networks.</figcaption>
</figure>

<p><em>Local receptive fields</em>: Given a 28x28 image as the input layer, instead of directly connecting to all 28x28 pixels, the convolutional layer only connects to small, localized regions (local receptive fields) of the input image in the hope of detecting some localized features. In this example, the small region size is 5x5, so exploring all those regions with a stride length of one results into 24x24 neurons. Here we have three feature maps, so the convolutional layer eventually has 3x24x24 neurons.</p>
<p><em>Shared weights and biases</em>: We use the same weights and bias for each neuron in a 24x24 feature map. It makes sense to use the same parameters in detecting the same feature but only in different local regions, and it can greatly reduce the number of parameters involved in a convolutional network compared to a fully connected layer.</p>
<p><em>The pooling layer</em>: A pooling layer is usually used immediately after a convolutional layer. What the pooling layer do is simplify the information in the output from the convolutional layer. In this example, each unit in the pooling layer summarizes a region of 2x2 in the previous convolutional layer, which results into 3x12x12 neurons. The pooling can be a max-pooling (finding the maximum activation of the region to be summarized) or L2 pooling (obtaining the square root of the sum of the squares of the activations in the region to be summarized).</p>
<p>To summarize, a convolutional network tries to detect localized features and at the same time greatly reduces the number of parameters. In principle, we could include any number of convolutional/pooling layers and also fully connected layers in our networks. What is exciting is that the backpropagation algorithm can apply to convolutional networks with only necessary minor modifications.</p>
<p>Does convolutional networks avoid the vanishing/exploding gradient problem? Not really, but it helps us to proceed anyway, e.g., by reducing the number of parameters. The problem can be considerably alleviated by convolutional/pooling layers as well as other techniques, such as powerful regularization, advanced artificial neurons and more training epochs by GPU.</p>
<h4>Deep Learning in Practice</h4>
<p>It may be not that hard to construct a very basic (deep) NN and achieve a nice performance. However, there are still many techniques that could help to improve the NN's performance. A couple of insightful ideas are listed below, which should be kept in mind if we are working towards a better NN.</p>
<ul>
<li>Use more convolutional/pooling layers: reduce the number of parameters and make learning faster;</li>
<li>Use <a href="http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function">right cost functions</a>: resolve the problem of learning slow down by the cross-entropy cost function;</li>
<li>Use <a href="http://neuralnetworksanddeeplearning.com/chap3.html#overfitting_and_regularization">regularization</a>: combat overfitting by L2/L1/drop-out regularization;</li>
<li>Use <a href="http://neuralnetworksanddeeplearning.com/chap3.html#other_models_of_artificial_neuron">advanced neurons</a>: for example, tanh neurons and rectified linear units;</li>
<li>Use <a href="http://neuralnetworksanddeeplearning.com/chap3.html#weight_initialization">good weight initialization</a>: avoid neuron saturation and learn faster;</li>
<li>Use an expanded training data set: for example, rotate/shift images as new training data in image classification;</li>
<li>Use an ensemble of NNs: heavy computation, but more models beat one;</li>
<li>Use GPU: gain more training epochs;</li>
</ul>
<h3>Acknowledgement</h3>
<p>A large majority of this post comes from Michael Nielsen's book<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup> entitled "neural networks and deep learning", which I would strongly recommend to anyone interested in discovering how essentially neural networks work.</p>
<h3>References</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>M. Nielsen, <em><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks and Deep Learning</a></em>, Determination Press, 2015.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div> -->

    <div class='article'>
        <a href="https://stlong0521.github.io/20160403 - NN and DL.html"><h2>Neural Networks and Deep Learning</h2></a>
        <div class= "well small"> Sun 03 Apr 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  </div>
        <div class="summary"><p>It has been a long time since the idea of neural networks was proposed, but it is really during the last few years that neural networks have become widely used. One of the major enablers is the infrastructure with high computational capability (e.g., cloud computing), which makes the training ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160403 - NN and DL.html">read more</a></div>
    </div>
	

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160326 - LDA.html"><h2>Latent Dirichlet Allocation and Topic Modeling</h2></a>
        <div class= "well small"> Sat 26 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  </div>
        <div class="summary"><p>When reading an article, we humans are able to easily identify the topics the article talks about. An interesting question is: can we automate this process, i.e., train a machine to find out the underlying topics in articles? In this post, a very popular topic modeling method, Latent Dirichlet ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160326 - LDA.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160319 - HMM and POS.html"><h2>Hidden Markov Model and Part of Speech Tagging</h2></a>
        <div class= "well small"> Sat 19 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  </div>
        <div class="summary"><p>In a Markov model, we generally assume that the states are directly observable or one state corresponds to one observation/event only. However, this is not always true. A good example would be: in speech recognition, we are supposed to identify a sequence of words given a sequence of utterances ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160319 - HMM and POS.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160312 - EM and GMM.html"><h2>Expectation Maximization Algorithm and Gaussian Mixture Model</h2></a>
        <div class= "well small"> Sat 12 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  </div>
        <div class="summary"><p>In statistical modeling, it is possible that some observations are just missing. For example, when flipping two biased coins with unknown biases, we only have a sequence of observations on heads and tails, but forgot to record which coin each observation comes from. In this case, the conventional maximum likelihood ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160312 - EM and GMM.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160305 - Missing Word.html"><h2>Locating and Filling Missing Words in Sentences</h2></a>
        <div class= "well small"> Sat 05 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a>  </div>
        <div class="summary"><p>There has been many occasions that we have incomplete sentences that are needed to completed. One example is that in speech recognition noisy environment can lead to unrecognizable words, but we still hope to recover and understand the complete sentence (e.g., by inference); another example is sentence completion questions ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160305 - Missing Word.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160228 - Logistic Regression.html"><h2>Binary and Multiclass Logistic Regression Classifiers</h2></a>
        <div class= "well small"> Sun 28 Feb 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a> <a href="https://stlong0521.github.io/tag/data-mining.html">Data Mining</a>  </div>
        <div class="summary"><p>The generative classification model, such as Naive Bayes, tries to learn the probabilities and then predict by using Bayes rules to calculate the posterior, <span class="math">\(p(y|\textbf{x})\)</span>. However, discrimitive classifiers model the posterior directly. As one of the most popular discrimitive classifiers, logistic regression directly models the linear decision ...</p><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160228 - Logistic Regression.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/about.html"><h2>About Tianlong Song</h2></a>
        <div class= "well small"> Thu 25 Feb 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 </div>
        <div class="summary"><p>Tianlong Song is currently a PhD student in the Department of Electrical &amp; Computer Engineering at Michigan State University. His interests are primarily focused on machine learning, data mining, natural language processing and software engineering.</p>
<p>His blog keeps the records on how he moved forward little by little in these areas ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/about.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://stlong0521.github.io/author/tianlong-song.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://stlong0521.github.io/archives.html">Archives</a>
                <li><a href="https://stlong0521.github.io/tags.html">Tags</a>



                <li><a href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate">Atom feed</a></li>
                <li><a href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>

            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://stlong0521.github.io/category/machine-learning.html">Machine Learning</a></li>
                <li><a href="https://stlong0521.github.io/category/misc.html">misc</a></li>
                <li><a href="https://stlong0521.github.io/category/natural-language-processing.html">Natural Language Processing</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="/webpage/index.html">Tianlong's Webpage</a></li>
                <li><a href="/docs/Tianlong Song.pdf">Tianlong's Resume</a></li>
                <li><a href="https://zhwa.github.io/">Zhe Wang</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://www.linkedin.com/in/tianlongsong">LinkedIn</a></li>
                <li><a href="https://github.com/stlong0521">GitHub</a></li>
                <li><a href="https://www.facebook.com/tianlong.song.3">Facebook</a></li>
                <li><a href="https://twitter.com/stlong0521">Twitter</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://stlong0521.github.io">Tianlong's Blog</a> &copy; Tianlong Song 2016</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://stlong0521.github.io/theme/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-74544804-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>