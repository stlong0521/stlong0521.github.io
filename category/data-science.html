<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Tianlong's Blog - Data Science</title>
    <meta name="description" content="">
    <meta name="author" content="Tianlong Song">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://stlong0521.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://stlong0521.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/local.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://stlong0521.github.io">Tianlong's Blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about.html">About</a></li>
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


<!--     <div class='article'>
        <div class="content-title">
            <a href="https://stlong0521.github.io/20160312 - EM and GMM.html"><h1>Expectation Maximization Algorithm and Gaussian Mixture Model</h1></a>
Sat 12 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/data-science.html">Data Science</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  
        </div>
        
        <div><p>In statistical modeling, it is possible that some observations are just missing. For example, when flipping two biased coins with unknown biases, we only have a sequence of observations on heads and tails, but forgot to record which coin each observation comes from. In this case, the conventional maximum likelihood (ML) or maximum a posteriori (MAP) algorithm would no longer be able to work, and it is time for the expectation maximization (EM) algorithm to come into play.</p>
<h3>A Motivating Example</h3>
<p>Although the two-biased-coin example above works as a valid example, another example will be discussed here, as it is more relevant to practical needs. Let us assume that we have a collection of float numbers, which come from two different Gaussian distributions. Unfortunately, we do not know which distribution each number comes from. Now we are supposed to learn the two Gaussian distributions (i,e, their means and variances) from the given data. This is the well-known Gaussian mixture model (GMM). What makes things difficult is that we have missing observations, i.e., membership of each number towards the two distributions. Though conventional ML or MAP would not work here, this is a perfect problem that EM can handle.</p>
<h3>Expectation Maximization (EM) Algorithm<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></h3>
<p>Let us consider a statistical model with a vector of unknown parameters <span class="math">\(\boldsymbol\theta\)</span>, which generates a set of observed data <span class="math">\(\textbf{X}\)</span> and a set of missing observations <span class="math">\(\textbf{Z}\)</span>. The likelihood function, <span class="math">\(p(\textbf{X},\textbf{Z}|\boldsymbol\theta)\)</span>, characterizes the probability that <span class="math">\(\textbf{X}\)</span> and <span class="math">\(\textbf{Z}\)</span> appear given the model with parameters <span class="math">\(\boldsymbol\theta\)</span>. An intuitive idea to estimate <span class="math">\(\boldsymbol\theta\)</span> would be trying to perform the maximum likelihood estimation (MLE) considering all possible <span class="math">\(\textbf{Z}\)</span>, i.e.,
</p>
<div class="math">\begin{equation}
\max_{\boldsymbol\theta}{\ln{p(\textbf{X}|\boldsymbol\theta)}}=\max_{\boldsymbol\theta}{\ln{\sum_{\textbf{Z}}{p(\textbf{X},\textbf{Z}|\boldsymbol\theta)}}}=\max_{\boldsymbol\theta}{\ln{\sum_{\textbf{Z}}{p(\textbf{X}|\textbf{Z},\boldsymbol\theta)p(\textbf{Z}|\boldsymbol\theta)}}}.
\end{equation}</div>
<p>
Unfortunately, the problem above is not directly tractable, since we do not have any prior knowledge on the missing observations <span class="math">\(\textbf{Z}\)</span>.</p>
<p>The EM algorithm aims to solve the problem above by starting with a guess on <span class="math">\(\boldsymbol\theta=\boldsymbol\theta_{0}\)</span> and then iteratively applying the two steps as indicated below:</p>
<ul>
<li><em>Expectation Step (E Step):</em> Calculate the log likelihood with respect to <span class="math">\(\boldsymbol\theta\)</span> given <span class="math">\(\boldsymbol\theta_{t}\)</span> by
<div class="math">\begin{equation}
\mathcal{L}(\boldsymbol\theta|\boldsymbol\theta_{t})=\ln{\sum_{\textbf{Z}}{p(\textbf{X}|\textbf{Z},\boldsymbol\theta_{t})p(\textbf{Z}|\boldsymbol\theta_{t})}};
\end{equation}</div>
</li>
<li><em>Maximization Step (M Step):</em> Find the parameter vector that maximizes the log likelihood above and then update it as
<div class="math">\begin{equation}
\theta_{t+1}={\arg \, \max}_{\theta}{\mathcal{L}(\boldsymbol\theta|\boldsymbol\theta_{t})}.
\end{equation}</div>
</li>
</ul>
<p>There are two things that should be noted here:</p>
<ul>
<li>There are two categories of EM: <em>hard</em> EM and <em>soft</em> EM. The algorithm illustrated above is soft EM, because the log likelihood in the E step is weighted upon all possible <span class="math">\(\textbf{Z}\)</span> with their probabilities. While in hard EM, instead of using weighted average, we simply select the most probable <span class="math">\(\textbf{Z}\)</span> and then move forward. The <a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means algorithm</a> is a good example of hard EM algorithm.</li>
<li>The EM algorithm typically converges to a local optimum, and <em>cannot</em> guarantee global optimum. With this being said, the solution might differ with different initialization, and it is possibly helpful to try more than one initialization when applying EM practically.</li>
</ul>
<h3>Gaussian Mixture Model (GMM)</h3>
<p>In the motivating example, a GMM with two Gaussian distributions was introduced. Here we are going to extend it to a general case with <span class="math">\(K\)</span> Gaussian distributions, and the data points will be generalized to be multidimensional. At the same time, we will discuss how it can be used for clustering.</p>
<p>Given a data set containing <span class="math">\(N\)</span> data points, <span class="math">\(\mathcal{D}=\{\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_N\}\)</span>, in which each data point is a <span class="math">\(M\)</span>-dimensional column vector and comes from one of <span class="math">\(K\)</span> Gaussian distributions. Here we will introduce <span class="math">\(\mathcal{Z}=\{z_1,z_2,...,z_N\}\)</span> with <span class="math">\(z_i\in\{1,2,...,K\}\)</span> as latent (hidden) variables to represent the cluster membership of the data points in <span class="math">\(\mathcal{D}\)</span>. The <span class="math">\(K\)</span> Gaussian distributions are characterized by <span class="math">\(\mathcal{N}(\boldsymbol\mu_j,\boldsymbol\Sigma_j)\)</span> for <span class="math">\(j=1,2,...,K\)</span>, and the <span class="math">\(j\)</span>th distribution has a weight of <span class="math">\(\pi_j\)</span> accounted in the overall distribution. Let us first try to map this GMM model to the EM algorithm component by component:</p>
<ul>
<li><span class="math">\(\mathcal{D}\)</span> is the observed data;</li>
<li><span class="math">\(\mathcal{Z}\)</span> is the missing observations;</li>
<li><span class="math">\(\boldsymbol\mu_j\)</span>, <span class="math">\(\boldsymbol\Sigma_j\)</span> and <span class="math">\(\pi_j\)</span> are the unknown model parameters.</li>
</ul>
<p>Following the EM algorithm, we will start with a guess on the unknown parameters, and then iteratively applying E step and M step until convergence. In the E step, we calculate the log likelihood based on given model parameters by
</p>
<div class="math">\begin{align}
\begin{aligned}
LL&amp;=\ln{p(\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_N)}\\
&amp;=\ln{\prod_{i=1}^{N}{p(\textbf{x}_i)}}\\
&amp;=\ln{\prod_{i=1}^{N}{\sum_{j=1}^{K}{p(z_i=j)p(\textbf{x}_i|z_i=j)}}}\\
&amp;=\sum_{i=1}^{N}{\ln\left(\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}\right)}.
\end{aligned}
\end{align}</div>
<p>In the M step, we maximize the log likelihood by solving the optimization problem below:
</p>
<div class="math">\begin{align}
\begin{aligned}
\max_{\boldsymbol\mu_j,\boldsymbol\Sigma_j,\pi_j}~~&amp;{LL}\\
&amp;s.t.~\sum_{j=1}^{K}{\pi_j}=1.
\end{aligned}
\end{align}</div>
<p>
We can apply Lagrange multiplier to solve the problem above. Let
</p>
<div class="math">\begin{equation}
L=\sum_{i=1}^{N}{\ln\left(\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}\right)}-\lambda\left(\sum_{j=1}^{K}{\pi_j}-1\right),
\end{equation}</div>
<p>
where <span class="math">\(\lambda\)</span> is the Lagrange multiplier. Taking partial derivatives and setting them to zero, we can obtain the optimal parameters as below:
</p>
<div class="math">\begin{equation} \label{Eqn:Maximization_1}
\boldsymbol\mu_j=\frac{\sum_{i=1}^{N}{\gamma_{ij}}\textbf{x}_i}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}</div>
<div class="math">\begin{equation} \label{Eqn:Maximization_2}
\boldsymbol\Sigma_j=\frac{\sum_{i=1}^{N}{\gamma_{ij}}(\textbf{x}_i-\boldsymbol\mu_j)(\textbf{x}_i-\boldsymbol\mu_j)^T}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}</div>
<div class="math">\begin{equation} \label{Eqn:Maximization_3}
\pi_j=\frac{1}{N}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}</div>
<p>
where <span class="math">\(\gamma_{ij}=p(z_i=j|\textbf{x}_i)\)</span> is the cluster membership, which can be calculated using Bayes theorem,
</p>
<div class="math">\begin{equation}
\begin{split}
\gamma_{ij}&amp;=p(z_i=j|\textbf{x}_i)\\
&amp;=\frac{p(z_i=j)p(\textbf{x}_i|z_i=j)}{\sum_{j=1}^{K}{p(z_i=j)p(\textbf{x}_i|z_i=j)}}\\
&amp;=\frac{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}{\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}}.
\end{split}
\end{equation}</div>
<p>To summarize, the GMM model can be learned using EM algorithm as in the following steps:</p>
<ol>
<li>Initialize <span class="math">\(\boldsymbol\mu_j\)</span>, <span class="math">\(\boldsymbol\Sigma_j\)</span> and <span class="math">\(\pi_j\)</span> for <span class="math">\(j=1,2,...,K\)</span>;</li>
<li>Repeat the following two steps until the log likelihood converges:<ul>
<li>E Step: Estimate cluster membership <span class="math">\(\gamma_{ij}\)</span> by the equation right above for all data point <span class="math">\(x_i\)</span> and cluster <span class="math">\(z_i=j\)</span>;</li>
<li>M Step: Maximize the log likelihood and update the model parameters by (\ref{Eqn:Maximization_1})-(\ref{Eqn:Maximization_3}) based on cluster membership <span class="math">\(\gamma_{ij}\)</span>.</li>
</ul>
</li>
</ol>
<h3>References</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Wikipedia, <a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"><em>Expectationâ€“maximization algorithm</em></a>, accessed on Mar 12, 2016.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div> -->

    <div class='article'>
        <a href="https://stlong0521.github.io/20160312 - EM and GMM.html"><h2>Expectation Maximization Algorithm and Gaussian Mixture Model</h2></a>
        <div class= "well small"> Sat 12 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/data-science.html">Data Science</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  </div>
        <div class="summary"><p>In statistical modeling, it is possible that some observations are just missing. For example, when flipping two biased coins with unknown biases, we only have a sequence of observations on heads and tails, but forgot to record which coin each observation comes from. In this case, the conventional maximum likelihood ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160312 - EM and GMM.html">read more</a></div>
    </div>
	

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160228 - Logistic Regression.html"><h2>Binary and Multiclass Logistic Regression Classifiers</h2></a>
        <div class= "well small"> Sun 28 Feb 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/data-science.html">Data Science</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  </div>
        <div class="summary"><p>The generative classification model, such as Naive Bayes, tries to learn the probabilities and then predict by using Bayes rules to calculate the posterior, <span class="math">\(p(y|\textbf{x})\)</span>. However, discrimitive classifiers model the posterior directly. As one of the most popular discrimitive classifiers, logistic regression directly models the linear decision ...</p><script type='text/javascript'>if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160228 - Logistic Regression.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://stlong0521.github.io/category/data-science.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://stlong0521.github.io/archives.html">Archives</a>
                <li><a href="https://stlong0521.github.io/tags.html">Tags</a>




            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://stlong0521.github.io/category/data-science.html">Data Science</a></li>
                <li><a href="https://stlong0521.github.io/category/misc.html">misc</a></li>
                <li><a href="https://stlong0521.github.io/category/natural-language-processing.html">Natural Language Processing</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="/webpage/index.html">Tianlong's Webpage</a></li>
                <li><a href="/docs/Tianlong Song.pdf">Tianlong's Resume</a></li>
                <li><a href="https://zhwa.github.io/">Zhe Wang</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://www.linkedin.com/in/tianlongsong">LinkedIn</a></li>
                <li><a href="https://github.com/stlong0521">GitHub</a></li>
                <li><a href="https://www.facebook.com/tianlong.song.3">Facebook</a></li>
                <li><a href="https://twitter.com/stlong0521">Twitter</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://stlong0521.github.io">Tianlong's Blog</a> &copy; Tianlong Song 2016</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://stlong0521.github.io/theme/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-74544804-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>