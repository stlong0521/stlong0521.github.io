<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Tianlong's Blog - Natural Language Processing</title>
    <meta name="description" content="">
    <meta name="author" content="Tianlong Song">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://stlong0521.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://stlong0521.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/local.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->
        <link href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate" title="Tianlong's Blog" type="application/atom+xml">
        <link href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate" title="Tianlong's Blog" type="application/rss+xml">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://stlong0521.github.io">Tianlong's Blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about.html">About</a></li>
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


<!--     <div class='article'>
        <div class="content-title">
            <a href="https://stlong0521.github.io/20160326 - LDA.html"><h1>Latent Dirichlet Allocation and Topic Modeling</h1></a>
Sat 26 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  
        </div>
        
        <div><p>When reading an article, we humans are able to easily identify the topics the article talks about. An interesting question is: can we automate this process, i.e., train a machine to find out the underlying topics in articles? In this post, a very popular topic modeling method, Latent Dirichlet allocation (LDA), will be discussed.</p>
<h3>Latent Dirichlet Allocation (LDA) Topic Model<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></h3>
<p>Given a library of <span class="math">\(M\)</span> documents, <span class="math">\(\mathcal{L}=\{d_1,d_2,...,d_M\}\)</span>, where each document <span class="math">\(d_m\)</span> contains a sequence of words, <span class="math">\(d_m=\{w_{m,1},w_{m,2},...,w_{m,N_m}\}\)</span>, we need to think of a model which describes how essentially these documents are generated. Considering <span class="math">\(K\)</span> topics and a vocabulary <span class="math">\(V\)</span>, the LDA topic model assumes that the documents are generated by the following two steps:</p>
<ul>
<li>For each document <span class="math">\(d_m\)</span>, use a doc-to-topic model parameterized by <span class="math">\(\boldsymbol\vartheta_m\)</span> to generate the topic for the <span class="math">\(n\)</span>th word and denote it as <span class="math">\(z_{m,n}\)</span>, for all <span class="math">\(1 \leq n \leq N_m\)</span>;</li>
<li>For each generated topic <span class="math">\(k=z_{m,n}\)</span> corresponding to each word in each document, use a topic-to-word model parameterized by <span class="math">\(\boldsymbol\varphi_k\)</span> to generate the word <span class="math">\(w_{m,n}\)</span>.</li>
</ul>
<figure align="center">
<img src="/figures/20160326/LDA.png" alt="LDA Model">
<figcaption align="center">Fig. 1: LDA topic model.</figcaption>
</figure>

<p>The two steps are graphically illustrated in Fig. 1. Considering that the doc-to-topic model and the topic-to-word model essentially follow multinomial distributions (counts of each topic in a document or each word in a topic), a good prior for their parameters, <span class="math">\(\boldsymbol\vartheta_m\)</span> and <span class="math">\(\boldsymbol\varphi_k\)</span>, would be the conjugate prior of multinomial distribution, <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a>.</p>
<blockquote>
<p>A conjugate prior, <span class="math">\(p(\boldsymbol\varphi)\)</span>, of a likelihood, <span class="math">\(p(\textbf{x}|\boldsymbol\varphi)\)</span>, is a distribution that results in a posterior distribution, <span class="math">\(p(\boldsymbol\varphi|\textbf{x})\)</span> with the same functional form as the prior (but different parameters). For example, the conjugate prior of a multinomial distribution is Dirichlet distribution. That is, for a multinomial distribution parameterized by <span class="math">\(\boldsymbol\varphi\)</span>, if the prior for <span class="math">\(\boldsymbol\varphi\)</span> is a Dirichlet distribution characterized by <span class="math">\(Dir(\boldsymbol\varphi|\boldsymbol\alpha)\)</span>, after observing <span class="math">\(\textbf{x}\)</span>, the posterior for <span class="math">\(\boldsymbol\varphi\)</span> still follows a Dirichlet distribution <span class="math">\(Dir(\boldsymbol\varphi|\textbf{n}_x+\boldsymbol\alpha)\)</span>, but incorporating the counting result <span class="math">\(\textbf{n}_x\)</span> of observation <span class="math">\(\textbf{x}\)</span>.</p>
</blockquote>
<p>Keep this in mind, let us take a closer look at the two steps:</p>
<ol>
<li>In the first step, for the <span class="math">\(m\)</span>th document, assume the prior for the doc-to-topic model's parameter <span class="math">\(\boldsymbol\vartheta_m\)</span> follows <span class="math">\(Dir(\boldsymbol\vartheta_m|\boldsymbol\alpha)\)</span>, after observing topics in the document and obtaining the counting result <span class="math">\(\textbf{n}_m\)</span>, we have the posterior for <span class="math">\(\boldsymbol\vartheta_m\)</span> as <span class="math">\(Dir(\boldsymbol\vartheta_m|\textbf{n}_m+\boldsymbol\alpha)\)</span>. After some calculation, we can obtain the topic distribution for the <span class="math">\(m\)</span>th document as
<div class="math">\begin{equation}
p(\textbf{z}_m|\boldsymbol\alpha)=\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)},
\end{equation}</div>
where <span class="math">\(\Delta(\boldsymbol\alpha)\)</span> is the normalization factor for <span class="math">\(Dir(\textbf{p}|\boldsymbol\alpha)\)</span>, i.e., <span class="math">\(\Delta(\boldsymbol\alpha)=\int{\prod_{k=1}^K{p_k^{\alpha_k-1}}}d\textbf{p}\)</span>. Taking all documents into account,
<div class="math">\begin{equation} \label{Eqn:Doc2Topic}
p(\textbf{z}|\boldsymbol\alpha)=\prod_{m=1}^M{p(\textbf{z}_m|\boldsymbol\alpha)}=\prod_{m=1}^M{\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)}}.
\end{equation}</div>
</li>
<li>In the second step, similarly, for the <span class="math">\(k\)</span>th topic, assume the prior for the topic-to-word model's parameter <span class="math">\(\boldsymbol\varphi_k\)</span> follows <span class="math">\(Dir(\boldsymbol\varphi_k|\boldsymbol\beta)\)</span>, after observing words in the topic and obtaining the counting result <span class="math">\(\textbf{n}_k\)</span>, we have the posterior for <span class="math">\(\boldsymbol\varphi_k\)</span> as <span class="math">\(Dir(\boldsymbol\varphi_k|\textbf{n}_k+\boldsymbol\beta)\)</span>. After some calculation, we can obtain the word distribution for the <span class="math">\(k\)</span>th topic as
<div class="math">\begin{equation}
p(\textbf{w}_k|\textbf{z}_k,\boldsymbol\beta)=\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}.
\end{equation}</div>
Taking all topics into account,
<div class="math">\begin{equation} \label{Eqn:Topic2Word}
p(\textbf{w}|\textbf{z},\boldsymbol\beta)=\prod_{k=1}^K{p(\textbf{w}_k|\textbf{z}_k,\boldsymbol\beta)}=\prod_{k=1}^K{\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}}.
\end{equation}</div>
Combining (\ref{Eqn:Doc2Topic}) and (\ref{Eqn:Topic2Word}), we have
<div class="math">\begin{equation} \label{Eqn:Joint_Distribution}
p(\textbf{w},\textbf{z}|\boldsymbol\alpha,\boldsymbol\beta)=p(\textbf{w}|\textbf{z},\boldsymbol\beta)p(\textbf{z}|\boldsymbol\alpha)=\prod_{k=1}^K{\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}}\prod_{m=1}^M{\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)}}.
\end{equation}</div>
</li>
</ol>
<h3>Joint Distribution Emulation by Gibbs Sampling<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup></h3>
<p>So far we know that the documents can be characterized by a joint distribution of topics and words as shown in (\ref{Eqn:Joint_Distribution}). The words are given, but the associating topics are not. Now we are thinking how to properly associate a topic to each word in each document, such that the result will best fit the joint distribution in (\ref{Eqn:Joint_Distribution}). This is a typical problem that can be solved by <a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a>.</p>
<blockquote>
<p>Gibbs sampling, a special case of Monte Carlo Markov Chain (MCMC) sampling, is a method to emulate high-dimensional probability distributions <span class="math">\(p(\textbf{x})\)</span> by the stationary behaviour of a Markov chain. A typical Gibbs sampling works by: (i) Initialize <span class="math">\(\textbf{x}\)</span>; (ii) Repeat until convergence: for all <span class="math">\(i\)</span>, sample <span class="math">\(x_i\)</span> from <span class="math">\(p(x_i|\textbf{x}_{\neg i})\)</span>, where <span class="math">\(\neg i\)</span> indicates excluding the <span class="math">\(i\)</span>th dimension. According to the stationary behaviour of a Markov chain, a sufficiently large collection of samples after convergence would well approximate the desired distribution <span class="math">\(p(\textbf{x})\)</span>.</p>
</blockquote>
<p>In light of the observations above, to apply Gibbs sampling, it is essential to calculate <span class="math">\(p(x_i|\textbf{x}_{\neg i})\)</span>. In our case, it is <span class="math">\(p(z_i=k|\textbf{z}_{\neg i},\textbf{w})\)</span>, where <span class="math">\(i=(m,n)\)</span> is a two dimensional coordinate indicating the <span class="math">\(n\)</span>th word of the <span class="math">\(m\)</span>th document. Since <span class="math">\(z_i=k,w_i=t\)</span> involves the <span class="math">\(m\)</span>th document and the <span class="math">\(k\)</span>th topic only, <span class="math">\(p(z_i=k|\textbf{z}_{\neg i},\textbf{w})\)</span> eventually depends only on two probabilities: (i) the probability of document <span class="math">\(m\)</span> emitting topic <span class="math">\(k\)</span>, <span class="math">\(\hat{\vartheta}_{mk}\)</span>; (ii) the probability of topic <span class="math">\(k\)</span> emitting word <span class="math">\(t\)</span>, <span class="math">\(\hat{\varphi}_{kt}\)</span>. Formally,
</p>
<div class="math">\begin{equation} \label{Eqn:Gibbs_Sampling}
p(z_i=k|\textbf{z}_{\neg i},\textbf{w}) \propto \hat{\vartheta}_{mk}\hat{\varphi}_{kt}=\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^K{(n_{m,\neg i}^{(k)}+\alpha_k)}}\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^V{(n_{k,\neg i}^{(t)}+\beta_t)}},
\end{equation}</div>
<p>
where <span class="math">\(n_m^{(k)}\)</span> is the count of topic <span class="math">\(k\)</span> in document <span class="math">\(m\)</span>, <span class="math">\(n_k^{(t)}\)</span> is the count of word <span class="math">\(t\)</span> for topic <span class="math">\(k\)</span>, and <span class="math">\(\neg i\)</span> indicates that <span class="math">\(w_i\)</span> should not be counted. Besides, <span class="math">\(\alpha_k\)</span> and <span class="math">\(\beta_t\)</span> are the prior knowledge (pseudo counts) for topic <span class="math">\(k\)</span> and word <span class="math">\(t\)</span>, respectively. The underlying physical meaning is that (\ref{Eqn:Gibbs_Sampling}) actually characterizes a word-generating path <span class="math">\(p(topic~k|doc~m)p(word~t|topic~k)\)</span>.</p>
<h3>LDA: Training and Inference<sup id="fnref:2"><a class="footnote-ref" href="#fn:2" rel="footnote">2</a></sup></h3>
<p>With the LDA model built, we want to: (i) estimate the model parameters, <span class="math">\(\boldsymbol\vartheta_m\)</span> and <span class="math">\(\boldsymbol\varphi_k\)</span>, from training documents; (ii) find out the topic distribution, <span class="math">\(\boldsymbol\vartheta_{new}\)</span>, for each new document.</p>
<p>The training procedure is:</p>
<ol>
<li>Initialization: assign a topic to each word in each document randomly;</li>
<li>For each word in each document, update its topic by the Gibbs sampling equation (\ref{Eqn:Gibbs_Sampling});</li>
<li>Repeat 2 until the Gibbs sampling converges;</li>
<li>Calculate the topic-to-word model parameter by <span class="math">\(\hat{\varphi}_{kt}=\frac{n_k^{(t)}+\beta_t}{\sum_{v=1}^V{(n_k^{(t)}+\beta_t)}}\)</span>, and save them as the model parameters.</li>
</ol>
<p>Once the LDA model is trained, we are ready to analyze the topic distribution of any new document. The inference works in the following procedure:</p>
<ol>
<li>Initialization: assign a topic to each word in the new document randomly;</li>
<li>For each word in the new document, update its topic by the Gibbs sampling equation (\ref{Eqn:Gibbs_Sampling}) (Note that the <span class="math">\(\hat{\varphi}_{kt}\)</span> part is directly available from the trained model, and only <span class="math">\(\hat{\vartheta}_{mk}\)</span> needs to be calculated regarding the new document);</li>
<li>Repeat 2 until the Gibbs sampling converges;</li>
<li>Calculate the topic distribution by <span class="math">\(\hat{\vartheta}_{new,k}=\frac{n_{new}^{(k)}+\alpha_k}{\sum_{k=1}^K{(n_{new}^{(k)}+\alpha_k)}}\)</span>.</li>
</ol>
<p>There are multiple open-source LDA implementations available online. To learn how LDA could be implemented, a Python implementation can be found <a href="https://github.com/nrolland/pyLDA/blob/master/src/pyLDA.py">here</a>.</p>
<h3>LDA v.s. Probabilistic Latent Semantic Analysis (PLSA)</h3>
<p>PLSA is a maximum likelihood (ML) model, while LDA is a maximum a posterior (MAP) model (Bayesian estimation). With that said, LDA would reduce to PLSA if a uniform Dirichlet prior is used. LDA is actually more complex than PLSA, so what could be the key advantages of LDA? The answer is the PRIOR! LDA would defeat PLSA, if there is a good prior for the data and the data by itself is not sufficient to train a model well. </p>
<h3>References</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>G. Heinrich, <em><a href="http://www.arbylon.net/publications/text-est.pdf">Parameter estimation for text analysis</a></em>, 2005.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Z. Jin, <a href="http://cos.name/2013/03/lda-math-lda-text-modeling/"><em>LDA Topic Modeling (in Chinese)</em></a>,  accessed on Mar 26, 2016.&#160;<a class="footnote-backref" href="#fnref:2" rev="footnote" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div> -->

    <div class='article'>
        <a href="https://stlong0521.github.io/20160326 - LDA.html"><h2>Latent Dirichlet Allocation and Topic Modeling</h2></a>
        <div class= "well small"> Sat 26 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  </div>
        <div class="summary"><p>When reading an article, we humans are able to easily identify the topics the article talks about. An interesting question is: can we automate this process, i.e., train a machine to find out the underlying topics in articles? In this post, a very popular topic modeling method, Latent Dirichlet ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160326 - LDA.html">read more</a></div>
    </div>
	

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160319 - HMM and POS.html"><h2>Hidden Markov Model and Part of Speech Tagging</h2></a>
        <div class= "well small"> Sat 19 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  </div>
        <div class="summary"><p>In a Markov model, we generally assume that the states are directly observable or one state corresponds to one observation/event only. However, this is not always true. A good example would be: in speech recognition, we are supposed to identify a sequence of words given a sequence of utterances ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160319 - HMM and POS.html">read more</a></div>
    </div>	
				

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160305 - Missing Word.html"><h2>Locating and Filling Missing Words in Sentences</h2></a>
        <div class= "well small"> Sat 05 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a>  </div>
        <div class="summary"><p>There has been many occasions that we have incomplete sentences that are needed to completed. One example is that in speech recognition noisy environment can lead to unrecognizable words, but we still hope to recover and understand the complete sentence (e.g., by inference); another example is sentence completion questions ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160305 - Missing Word.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://stlong0521.github.io/category/natural-language-processing.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://stlong0521.github.io/archives.html">Archives</a>
                <li><a href="https://stlong0521.github.io/tags.html">Tags</a>



                <li><a href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate">Atom feed</a></li>
                <li><a href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>

            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://stlong0521.github.io/category/data-science.html">Data Science</a></li>
                <li><a href="https://stlong0521.github.io/category/misc.html">misc</a></li>
                <li><a href="https://stlong0521.github.io/category/natural-language-processing.html">Natural Language Processing</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="/webpage/index.html">Tianlong's Webpage</a></li>
                <li><a href="/docs/Tianlong Song.pdf">Tianlong's Resume</a></li>
                <li><a href="https://zhwa.github.io/">Zhe Wang</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://www.linkedin.com/in/tianlongsong">LinkedIn</a></li>
                <li><a href="https://github.com/stlong0521">GitHub</a></li>
                <li><a href="https://www.facebook.com/tianlong.song.3">Facebook</a></li>
                <li><a href="https://twitter.com/stlong0521">Twitter</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://stlong0521.github.io">Tianlong's Blog</a> &copy; Tianlong Song 2016</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://stlong0521.github.io/theme/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-74544804-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>