<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Tianlong's Blog - Natural Language Processing</title>
    <meta name="description" content="">
    <meta name="author" content="Tianlong Song">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://stlong0521.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://stlong0521.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/local.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://stlong0521.github.io">Tianlong's Blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about.html">About</a></li>
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
        

        


<!--     <div class='article'>
        <div class="content-title">
            <a href="https://stlong0521.github.io/20160319 - HMM and POS.html"><h1>Hidden Markov Model and Part of Speech Tagging</h1></a>
Sat 19 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  
        </div>
        
        <div><p>In a Markov model, we generally assume that the states are directly observable or one state corresponds to one observation/event only. However, this is not always true. A good example would be: in speech recognition, we are supposed to identify a sequence of words given a sequence of utterances, in which case the states (words) are not directly observable and one single state (word) could have different observations (utterances). This is a perfect example that could be treated as a hidden Markov model (HMM), by which the hidden states can be inferred from the observations.</p>
<h3>Elements of a Hidden Markov Model (HMM)<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></h3>
<p>A hidden Markov model, <span class="math">\(\Phi\)</span>, typically includes the following elements:</p>
<ul>
<li>Time: <span class="math">\(t=\{1,2,...,T\}\)</span>;</li>
<li><span class="math">\(N\)</span> States: <span class="math">\(Q=\{1,2,...,N\}\)</span>;</li>
<li><span class="math">\(M\)</span> Observations: <span class="math">\(O=\{1,2,...,M\}\)</span>;</li>
<li>Initial Probabilities: <span class="math">\(\pi_i=p(q_1=i),~1 \leq i \leq N\)</span>;</li>
<li>Transition Probabilities: <span class="math">\(a_{ij}=p(q_{t+1}=j|q_t=i),~1 \leq i,j \leq N\)</span>;</li>
<li>Observation Probabilities: <span class="math">\(b_j(k)=p(o_t=k|q_t=j)~1 \leq j \leq N, 1 \leq k \leq M\)</span>.</li>
</ul>
<p>The entire model can be characterized by <span class="math">\(\Phi=(A,B,\pi)\)</span>, where <span class="math">\(A=\{a_{ij}\}\)</span>, <span class="math">\(B=\{b_j(k)\}\)</span> and <span class="math">\(\pi=\{\pi_i\}\)</span>. The states are "hidden", since they are not directly observable, but reflected in observations with uncertainty.</p>
<h3>Three Basic Problems for HMMs<sup id="fnref:1"><a class="footnote-ref" href="#fn:1" rel="footnote">1</a></sup></h3>
<p>There are three basic problems that are very important to real-world applications of HMMs:</p>
<h4>Problem 1: Evaluation Problem</h4>
<blockquote>
<p>Given the observation sequence <span class="math">\(O=o_1o_2...o_T\)</span> and a model <span class="math">\(\Phi=(A,B,\pi)\)</span>, how to efficiently compute the probability of the observation sequence given the model, i.e., <span class="math">\(p(O|\Phi)\)</span>?</p>
</blockquote>
<p>Let
</p>
<div class="math">\begin{equation}
\alpha_t(i)=p(o_1o_2...o_t,q_t=i|\Phi)
\end{equation}</div>
<p>
denote the probability that the state is <span class="math">\(i\)</span> at time <span class="math">\(t\)</span> and we have a sequence of observations <span class="math">\(o_1o_2...o_t\)</span>. The evaluation problem can be solved by the forward algorithm as illustrated below:</p>
<ol>
<li>Base case: 
<div class="math">\begin{equation}
\alpha_1(i)=p(o_1,q_1=i|\Phi)=p(o_1|q_1=i,\Phi)p(q_1=i|\Phi)=\pi_ib_i(o_1),~1 \leq i \leq N;
\end{equation}</div>
</li>
<li>Induction:
<div class="math">\begin{equation}
\alpha_{t+1}(j)=\left[\sum_{i=1}^N{\alpha_{t}(i)a_{ij}}\right]b_j(o_{t+1}),~1 \leq j \leq N;
\end{equation}</div>
</li>
<li>Termination:
<div class="math">\begin{equation}
p(O|\Phi)=\sum_{i=1}^N{\alpha_T(i)},
\end{equation}</div>
<div class="math">\begin{equation}
p(q_T=i|O,\Phi)=\frac{\alpha_T(i)}{\sum_{j=1}^N{\alpha_T(j)}}.
\end{equation}</div>
</li>
</ol>
<p>The algorithm above essentially applies dynamic programming, and its complexity is <span class="math">\(O(N^2T)\)</span>.</p>
<h4>Problem 2: Decoding Problem</h4>
<blockquote>
<p>Given the observation sequence <span class="math">\(O=o_1o_2...o_T\)</span> and a model <span class="math">\(\Phi=(A,B,\pi)\)</span>, how to choose the "best" state sequence <span class="math">\(Q=q_1q_2...q_T\)</span> (the most probable path) in terms of how good it explains the observations?</p>
</blockquote>
<p>Define
</p>
<div class="math">\begin{equation}
v_t(i)=\max_{q_1q_2...q_{t-1}}{p(q_1q_2...q_{t-1},q_t=i,o_1o_2...o_t|\Phi)}
\end{equation}</div>
<p>
as the best state sequence through which the state arrives at <span class="math">\(i\)</span> at time <span class="math">\(t\)</span> with a sequence of observations <span class="math">\(o_1o_2...o_t\)</span>. The decoding problem can be solved by the Viterbi algorithm as illustrated below:</p>
<ol>
<li>Base case: 
<div class="math">\begin{equation}
v_1(i)=p(q_1=i,o_1|\Phi)=p(o_1|q_1=i,\Phi)p(q_1=i|\Phi)=\pi_ib_i(o_1),~1 \leq i \leq N;
\end{equation}</div>
</li>
<li>Induction:
<div class="math">\begin{equation}
v_{t+1}(j)=\left[\max_i{v_{t}(i)a_{ij}}\right]b_j(o_{t+1}),~1 \leq j \leq N,
\end{equation}</div>
in which the optimal <span class="math">\(i\)</span> from the maximization should be stored properly for backtracking;</li>
<li>Termination: The best state sequence can be determined by first finding the optimal final state
<div class="math">\begin{equation}
q_T=\max_i{v_T(i)},
\end{equation}</div>
and then backtracking all the way to the initial state.</li>
</ol>
<p>The algorithm above also applies dynamic programming, and its complexity is <span class="math">\(O(N^2T)\)</span> as well.</p>
<h4>Problem 3: Model Learning</h4>
<blockquote>
<p>Given the observation sequence <span class="math">\(O=o_1o_2...o_T\)</span>, how to find the model <span class="math">\(\Phi=(A,B,\pi)\)</span> that maximizes <span class="math">\(p(O|\Phi)\)</span>?</p>
</blockquote>
<p>A general maximum likelihood (ML) learning approach could determine the optimal <span class="math">\(\Phi\)</span> as
</p>
<div class="math">\begin{equation}
\hat{\Phi}=\max_{\Phi}{p(O|\Phi)}.
\end{equation}</div>
<p>
It is much easier to perform supervised learning, where the true state are tagged to each observation. Given <span class="math">\(V\)</span> training sequences in total, the model parameters can be estimated as
</p>
<div class="math">\begin{equation} \label{Eqn:Supervised_Learning}
\hat{a}_{ij}=\frac{Count(q:i \rightarrow j)}{Count(q:i)},~~\hat{b}_j(k)=\frac{Count(q:j,o:k)}{Count(q:j)},~~\hat{\pi}_i=\frac{Count(q_1=i)}{V}.
\end{equation}</div>
<p>It becomes a little bit tricky for unsupervised learning, where the true state are not tagged. To facilitate our model learning, we need to first introduce the following definition/calculation:
</p>
<div class="math">\begin{equation} \label{Eqn:Episilon}
\varepsilon_t(i,j)=p(q_t=i,q_{t+1}=j|O,\Phi)=\frac{p(q_t=i,q_{t+1}=j,O|\Phi)}{p(O|\Phi)}=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N{\alpha_T(i)}},
\end{equation}</div>
<p>
where <span class="math">\(p(O|\Phi)\)</span> is exactly Problem 1 we have yet talked about. <span class="math">\(\beta_{t+1}(j)\)</span> can be calculated using the backward algorithm, which is very similar to the forward algorithm in Problem 1 to calculate <span class="math">\(\alpha_t(i)\)</span> except the difference in the direction of calculation. Following (\ref{Eqn:Episilon}), we further introduce
</p>
<div class="math">\begin{equation} \label{Eqn:Gamma}
\gamma_t(i)=p(q_t=i|O,\Phi)=\sum_{j=1}^N{\varepsilon_t(i,j)}.
\end{equation}</div>
<p>
Then the model parameters can be recomputed as
</p>
<div class="math">\begin{equation}
\begin{split}
\hat{a}_{ij}&amp;=\frac{Expected~number~of~transitions~from~state~i~to~j}{Expected~number~of~transitions~from~state~i}\\
&amp;=\frac{\sum_{t=1}^{T-1}{\varepsilon_t(i,j)}}{\sum_{t=1}^{T-1}{\gamma_t(i)}},
\end{split}
\end{equation}</div>
<div class="math">\begin{equation}
\begin{split}
\hat{b}_j(k)&amp;=\frac{Expected~number~of~times~in~state~j~and~observing~k}{Expected~number~of~times~in~state~j}\\
&amp;=\frac{\sum_{t=1,~s.t.~o_t=k}^{T}{\gamma_t(j)}}{\sum_{t=1}^{T}{\gamma_t(j)}},
\end{split}
\end{equation}</div>
<div class="math">\begin{equation}
\begin{split}
\hat{\pi}_i&amp;=Expected~number~of~times~in~state~i~at~time~t=1\\
&amp;=\gamma_1(i).
\end{split}
\end{equation}</div>
<p>Now we are ready to apply the <a href="20160312 - EM and GMM.html">expectation maximization (EM) algorithm</a> for HMM learning. More specifically:</p>
<ol>
<li>Initialize the HMM, <span class="math">\(\Phi\)</span>;</li>
<li>Repeat the two steps below until convergence:<ul>
<li>E Step: Given observations <span class="math">\(o_1o_2...o_T\)</span> and the model <span class="math">\(\Phi\)</span>, compute <span class="math">\(\varepsilon_t(i,j)\)</span> by (\ref{Eqn:Episilon}) and <span class="math">\(\gamma_t(i)\)</span> by (\ref{Eqn:Gamma});</li>
<li>M Step: Update the model <span class="math">\(\Phi\)</span> by recomputing parameters using the three equations right above.</li>
</ul>
</li>
</ol>
<h3>Part of Speech (POS) Tagging</h3>
<p>In natural language processing, part of speech (POS) tagging is to associate with each word in a sentence a lexical tag. As an example, Janet (NNP) will (MD) back (VB) the (DT) bill (NN), in which each POS tag describes what its corresponding word is about. In this particular example, "VB" tells that "back" is a verb, and "NN" tells that "bill" is a noun, etc.</p>
<p>POS tagging is very useful, because it is usually the first step of many practical tasks, e.g., speech synthesis, grammatical parsing and information extraction. For instance, if we want to pronounce the word "record" correctly, we need to first learn from context if it is a noun or verb and then determine where the stress is in its pronunciation. A similar argument applies to grammatical parsing and information extraction as well.</p>
<p>We need to do some preprocessing before performing POS tagging using HMM. First, because the vocabulary size could be very large while most of the words are not frequently used, we replace each low-frequency word with a special word "UNKA". This is very helpful to reduce the vocabulary size, and thus reduce the memory cost on storing the probability matrix. Second, for each sentence, we add two tags to represent sentence boundaries, e.g., "START" and "END".</p>
<p>Now we are ready to apply HMM to perform POS tagging. The model can be characterized by:</p>
<ul>
<li>Time: length of each sentence;</li>
<li><span class="math">\(N\)</span> States: POS tags, e.g., 45 POS tags from Penn Treebank;</li>
<li><span class="math">\(M\)</span> Observations: vocabulary (compressed by replacing low-frequency words with "UNKA");</li>
<li>Initial Probabilities: probability of each tag associated to the first word;</li>
<li>Transition Probabilities: <span class="math">\(p(t_{i+1}|t_i)\)</span>, where <span class="math">\(t_i\)</span> represents the tag for the <span class="math">\(i\)</span>th word;</li>
<li>Observation Probabilities: <span class="math">\(p(w|t)\)</span>, where <span class="math">\(t\)</span> stands for a tag and <span class="math">\(w\)</span> stands for a word.</li>
</ul>
<p>Once we finish training the model, e.g., under supervised learning by (\ref{Eqn:Supervised_Learning}), we will then be able to tag new sentences applying the Viterbi algorithm as previously illustrated in Problem 2 for HMM. To see details about implementing POS tagging using HMM, <a href="https://github.com/stlong0521/hmm-pos">click here</a> for demo codes.</p>
<h3>References</h3>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>L. R. Rabiner, "A tutorial on hidden Markov models and selected applications in speech recognition," in Proceedings of the IEEE, vol. 77, no. 2, pp. 257-286, Feb 1989.&#160;<a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
        <hr />
    </div> -->

    <div class='article'>
        <a href="https://stlong0521.github.io/20160319 - HMM and POS.html"><h2>Hidden Markov Model and Part of Speech Tagging</h2></a>
        <div class= "well small"> Sat 19 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a> <a href="https://stlong0521.github.io/tag/machine-learning.html">Machine Learning</a>  </div>
        <div class="summary"><p>In a Markov model, we generally assume that the states are directly observable or one state corresponds to one observation/event only. However, this is not always true. A good example would be: in speech recognition, we are supposed to identify a sequence of words given a sequence of utterances ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160319 - HMM and POS.html">read more</a></div>
    </div>
	

 
        

 

    <div class='article'>
        <a href="https://stlong0521.github.io/20160305 - Missing Word.html"><h2>Locating and Filling Missing Words in Sentences</h2></a>
        <div class= "well small"> Sat 05 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a>  </div>
        <div class="summary"><p>There has been many occasions that we have incomplete sentences that are needed to completed. One example is that in speech recognition noisy environment can lead to unrecognizable words, but we still hope to recover and understand the complete sentence (e.g., by inference); another example is sentence completion questions ...</p> <a class="btn btn-info xsmall" href="https://stlong0521.github.io/20160305 - Missing Word.html">read more</a></div>
    </div>	
				
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://stlong0521.github.io/category/natural-language-processing.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>
 
  
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://stlong0521.github.io/archives.html">Archives</a>
                <li><a href="https://stlong0521.github.io/tags.html">Tags</a>




            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://stlong0521.github.io/category/data-science.html">Data Science</a></li>
                <li><a href="https://stlong0521.github.io/category/misc.html">misc</a></li>
                <li><a href="https://stlong0521.github.io/category/natural-language-processing.html">Natural Language Processing</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="/webpage/index.html">Tianlong's Webpage</a></li>
                <li><a href="/docs/Tianlong Song.pdf">Tianlong's Resume</a></li>
                <li><a href="https://zhwa.github.io/">Zhe Wang</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://www.linkedin.com/in/tianlongsong">LinkedIn</a></li>
                <li><a href="https://github.com/stlong0521">GitHub</a></li>
                <li><a href="https://www.facebook.com/tianlong.song.3">Facebook</a></li>
                <li><a href="https://twitter.com/stlong0521">Twitter</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://stlong0521.github.io">Tianlong's Blog</a> &copy; Tianlong Song 2016</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://stlong0521.github.io/theme/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-74544804-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>