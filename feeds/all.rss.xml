<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tianlong's Blog</title><link>https://stlong0521.github.io/</link><description></description><atom:link href="https://stlong0521.github.io/feeds/all.rss.xml" rel="self"></atom:link><lastBuildDate>Sat, 26 Mar 2016 00:00:00 -0400</lastBuildDate><item><title>Latent Dirichlet Allocation and Topic Modeling</title><link>https://stlong0521.github.io/20160326%20-%20LDA.html</link><description>&lt;p&gt;When reading an article, we humans are able to easily identify the topics the article talks about. An interesting question is: can we automate this process, i.e., train a machine to find out the underlying topics in articles? In this post, a very popular topic modeling method, Latent Dirichlet allocation (LDA), will be discussed.&lt;/p&gt;
&lt;h3&gt;Latent Dirichlet Allocation (LDA) Topic Model&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Given a library of &lt;span class="math"&gt;\(M\)&lt;/span&gt; documents, &lt;span class="math"&gt;\(\mathcal{L}=\{d_1,d_2,...,d_M\}\)&lt;/span&gt;, where each document &lt;span class="math"&gt;\(d_m\)&lt;/span&gt; contains a sequence of words, &lt;span class="math"&gt;\(d_m=\{w_{m,1},w_{m,2},...,w_{m,N_m}\}\)&lt;/span&gt;, we need to think of a model which describes how essentially these documents are generated. Considering &lt;span class="math"&gt;\(K\)&lt;/span&gt; topics and a vocabulary &lt;span class="math"&gt;\(V\)&lt;/span&gt;, the LDA topic model assumes that the documents are generated by the following two steps:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For each document &lt;span class="math"&gt;\(d_m\)&lt;/span&gt;, use a doc-to-topic model parameterized by &lt;span class="math"&gt;\(\boldsymbol\vartheta_m\)&lt;/span&gt; to generate the topic for the &lt;span class="math"&gt;\(n\)&lt;/span&gt;th word and denote it as &lt;span class="math"&gt;\(z_{m,n}\)&lt;/span&gt;, for all &lt;span class="math"&gt;\(1 \leq n \leq N_m\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;For each generated topic &lt;span class="math"&gt;\(k=z_{m,n}\)&lt;/span&gt; corresponding to each word in each document, use a topic-to-word model parameterized by &lt;span class="math"&gt;\(\boldsymbol\varphi_k\)&lt;/span&gt; to generate the word &lt;span class="math"&gt;\(w_{m,n}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure align="center"&gt;
&lt;img src="/figures/20160326/LDA.png" alt="LDA Model"&gt;
&lt;figcaption align="center"&gt;Fig. 1: LDA topic model.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The two steps are graphically illustrated in Fig. 1. Considering that the doc-to-topic model and the topic-to-word model essentially follow multinomial distributions (counts of each topic in a document or each word in a topic), a good prior for their parameters, &lt;span class="math"&gt;\(\boldsymbol\vartheta_m\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol\varphi_k\)&lt;/span&gt;, would be the conjugate prior of multinomial distribution, &lt;a href="https://en.wikipedia.org/wiki/Dirichlet_distribution"&gt;Dirichlet distribution&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A conjugate prior, &lt;span class="math"&gt;\(p(\boldsymbol\varphi)\)&lt;/span&gt;, of a likelihood, &lt;span class="math"&gt;\(p(\textbf{x}|\boldsymbol\varphi)\)&lt;/span&gt;, is a distribution that results in a posterior distribution, &lt;span class="math"&gt;\(p(\boldsymbol\varphi|\textbf{x})\)&lt;/span&gt; with the same functional form as the prior (but different parameters). For example, the conjugate prior of a multinomial distribution is Dirichlet distribution. That is, for a multinomial distribution parameterized by &lt;span class="math"&gt;\(\boldsymbol\varphi\)&lt;/span&gt;, if the prior for &lt;span class="math"&gt;\(\boldsymbol\varphi\)&lt;/span&gt; is a Dirichlet distribution characterized by &lt;span class="math"&gt;\(Dir(\boldsymbol\varphi|\boldsymbol\alpha)\)&lt;/span&gt;, after observing &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, the posterior for &lt;span class="math"&gt;\(\boldsymbol\varphi\)&lt;/span&gt; still follows a Dirichlet distribution &lt;span class="math"&gt;\(Dir(\boldsymbol\varphi|\textbf{n}_x+\boldsymbol\alpha)\)&lt;/span&gt;, but incorporating the counting result &lt;span class="math"&gt;\(\textbf{n}_x\)&lt;/span&gt; of observation &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Keep this in mind, let us take a closer look at the two steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the first step, for the &lt;span class="math"&gt;\(m\)&lt;/span&gt;th document, assume the prior for the doc-to-topic model's parameter &lt;span class="math"&gt;\(\boldsymbol\vartheta_m\)&lt;/span&gt; follows &lt;span class="math"&gt;\(Dir(\boldsymbol\vartheta_m|\boldsymbol\alpha)\)&lt;/span&gt;, after observing topics in the document and obtaining the counting result &lt;span class="math"&gt;\(\textbf{n}_m\)&lt;/span&gt;, we have the posterior for &lt;span class="math"&gt;\(\boldsymbol\vartheta_m\)&lt;/span&gt; as &lt;span class="math"&gt;\(Dir(\boldsymbol\vartheta_m|\textbf{n}_m+\boldsymbol\alpha)\)&lt;/span&gt;. After some calculation, we can obtain the topic distribution for the &lt;span class="math"&gt;\(m\)&lt;/span&gt;th document as
&lt;div class="math"&gt;\begin{equation}
p(\textbf{z}_m|\boldsymbol\alpha)=\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)},
\end{equation}&lt;/div&gt;
where &lt;span class="math"&gt;\(\Delta(\boldsymbol\alpha)\)&lt;/span&gt; is the normalization factor for &lt;span class="math"&gt;\(Dir(\textbf{p}|\boldsymbol\alpha)\)&lt;/span&gt;, i.e., &lt;span class="math"&gt;\(\Delta(\boldsymbol\alpha)=\int{\prod_{k=1}^K{p_k^{\alpha_k-1}}}d\textbf{p}\)&lt;/span&gt;. Taking all documents into account,
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Doc2Topic}
p(\textbf{z}|\boldsymbol\alpha)=\prod_{m=1}^M{p(\textbf{z}_m|\boldsymbol\alpha)}=\prod_{m=1}^M{\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)}}.
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;In the second step, similarly, for the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th topic, assume the prior for the topic-to-word model's parameter &lt;span class="math"&gt;\(\boldsymbol\varphi_k\)&lt;/span&gt; follows &lt;span class="math"&gt;\(Dir(\boldsymbol\varphi_k|\boldsymbol\beta)\)&lt;/span&gt;, after observing words in the topic and obtaining the counting result &lt;span class="math"&gt;\(\textbf{n}_k\)&lt;/span&gt;, we have the posterior for &lt;span class="math"&gt;\(\boldsymbol\varphi_k\)&lt;/span&gt; as &lt;span class="math"&gt;\(Dir(\boldsymbol\varphi_k|\textbf{n}_k+\boldsymbol\beta)\)&lt;/span&gt;. After some calculation, we can obtain the word distribution for the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th topic as
&lt;div class="math"&gt;\begin{equation}
p(\textbf{w}_k|\textbf{z}_k,\boldsymbol\beta)=\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}.
\end{equation}&lt;/div&gt;
Taking all topics into account,
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Topic2Word}
p(\textbf{w}|\textbf{z},\boldsymbol\beta)=\prod_{k=1}^K{p(\textbf{w}_k|\textbf{z}_k,\boldsymbol\beta)}=\prod_{k=1}^K{\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}}.
\end{equation}&lt;/div&gt;
Combining (\ref{Eqn:Doc2Topic}) and (\ref{Eqn:Topic2Word}), we have
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Joint_Distribution}
p(\textbf{w},\textbf{z}|\boldsymbol\alpha,\boldsymbol\beta)=p(\textbf{w}|\textbf{z},\boldsymbol\beta)p(\textbf{z}|\boldsymbol\alpha)=\prod_{k=1}^K{\frac{\Delta(\textbf{n}_k+\boldsymbol\beta)}{\Delta(\boldsymbol\beta)}}\prod_{m=1}^M{\frac{\Delta(\textbf{n}_m+\boldsymbol\alpha)}{\Delta(\boldsymbol\alpha)}}.
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Joint Distribution Emulation by Gibbs Sampling&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;So far we know that the documents can be characterized by a joint distribution of topics and words as shown in (\ref{Eqn:Joint_Distribution}). The words are given, but the associating topics are not. Now we are thinking how to properly associate a topic to each word in each document, such that the result will best fit the joint distribution in (\ref{Eqn:Joint_Distribution}). This is a typical problem that can be solved by &lt;a href="https://en.wikipedia.org/wiki/Gibbs_sampling"&gt;Gibbs sampling&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Gibbs sampling, a special case of Monte Carlo Markov Chain (MCMC) sampling, is a method to emulate high-dimensional probability distributions &lt;span class="math"&gt;\(p(\textbf{x})\)&lt;/span&gt; by the stationary behaviour of a Markov chain. A typical Gibbs sampling works by: (i) Initialize &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;; (ii) Repeat until convergence: for all &lt;span class="math"&gt;\(i\)&lt;/span&gt;, sample &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; from &lt;span class="math"&gt;\(p(x_i|\textbf{x}_{\neg i})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\neg i\)&lt;/span&gt; indicates excluding the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th dimension. According to the stationary behaviour of a Markov chain, a sufficiently large collection of samples after convergence would well approximate the desired distribution &lt;span class="math"&gt;\(p(\textbf{x})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In light of the observations above, to apply Gibbs sampling, it is essential to calculate &lt;span class="math"&gt;\(p(x_i|\textbf{x}_{\neg i})\)&lt;/span&gt;. In our case, it is &lt;span class="math"&gt;\(p(z_i=k|\textbf{z}_{\neg i},\textbf{w})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(i=(m,n)\)&lt;/span&gt; is a two dimensional coordinate indicating the &lt;span class="math"&gt;\(n\)&lt;/span&gt;th word of the &lt;span class="math"&gt;\(m\)&lt;/span&gt;th document. Since &lt;span class="math"&gt;\(z_i=k,w_i=t\)&lt;/span&gt; involves the &lt;span class="math"&gt;\(m\)&lt;/span&gt;th document and the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th topic only, &lt;span class="math"&gt;\(p(z_i=k|\textbf{z}_{\neg i},\textbf{w})\)&lt;/span&gt; eventually depends only on two probabilities: (i) the probability of document &lt;span class="math"&gt;\(m\)&lt;/span&gt; emitting topic &lt;span class="math"&gt;\(k\)&lt;/span&gt;, &lt;span class="math"&gt;\(\hat{\vartheta}_{mk}\)&lt;/span&gt;; (ii) the probability of topic &lt;span class="math"&gt;\(k\)&lt;/span&gt; emitting word &lt;span class="math"&gt;\(t\)&lt;/span&gt;, &lt;span class="math"&gt;\(\hat{\varphi}_{kt}\)&lt;/span&gt;. Formally,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Gibbs_Sampling}
p(z_i=k|\textbf{z}_{\neg i},\textbf{w}) \propto \hat{\vartheta}_{mk}\hat{\varphi}_{kt}=\frac{n_{m,\neg i}^{(k)}+\alpha_k}{\sum_{k=1}^K{(n_{m,\neg i}^{(k)}+\alpha_k)}}\frac{n_{k,\neg i}^{(t)}+\beta_t}{\sum_{t=1}^V{(n_{k,\neg i}^{(t)}+\beta_t)}},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(n_m^{(k)}\)&lt;/span&gt; is the count of topic &lt;span class="math"&gt;\(k\)&lt;/span&gt; in document &lt;span class="math"&gt;\(m\)&lt;/span&gt;, &lt;span class="math"&gt;\(n_k^{(t)}\)&lt;/span&gt; is the count of word &lt;span class="math"&gt;\(t\)&lt;/span&gt; for topic &lt;span class="math"&gt;\(k\)&lt;/span&gt;, and &lt;span class="math"&gt;\(\neg i\)&lt;/span&gt; indicates that &lt;span class="math"&gt;\(w_i\)&lt;/span&gt; should not be counted. Besides, &lt;span class="math"&gt;\(\alpha_k\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta_t\)&lt;/span&gt; are the prior knowledge (pseudo counts) for topic &lt;span class="math"&gt;\(k\)&lt;/span&gt; and word &lt;span class="math"&gt;\(t\)&lt;/span&gt;, respectively. The underlying physical meaning is that (\ref{Eqn:Gibbs_Sampling}) actually characterizes a word-generating path &lt;span class="math"&gt;\(p(topic~k|doc~m)p(word~t|topic~k)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;LDA: Training and Inference&lt;/h3&gt;
&lt;p&gt;With the LDA model built, we want to: (i) estimate the model parameters, &lt;span class="math"&gt;\(\boldsymbol\vartheta_m\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol\varphi_k\)&lt;/span&gt;, from training documents; (ii) find out the topic distribution, &lt;span class="math"&gt;\(\boldsymbol\vartheta_{new}\)&lt;/span&gt;, for each new document.&lt;/p&gt;
&lt;p&gt;The training procedure is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialization: assign a topic to each word in each document randomly;&lt;/li&gt;
&lt;li&gt;For each word in each document, update its topic by the Gibbs sampling equation (\ref{Eqn:Gibbs_Sampling});&lt;/li&gt;
&lt;li&gt;Repeat 2 until the Gibbs sampling converges;&lt;/li&gt;
&lt;li&gt;Calculate the topic-to-word model parameter by &lt;span class="math"&gt;\(\hat{\varphi}_{kt}=\frac{n_k^{(t)}+\beta_t}{\sum_{v=1}^V{(n_k^{(t)}+\beta_t)}}\)&lt;/span&gt;, and save them as the model parameters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once the LDA model is trained, we are ready to analyze the topic distribution of any new document. The inference works in the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialization: assign a topic to each word in the new document randomly;&lt;/li&gt;
&lt;li&gt;For each word in the new document, update its topic by the Gibbs sampling equation (\ref{Eqn:Gibbs_Sampling}) (Note that the &lt;span class="math"&gt;\(\hat{\varphi}_{kt}\)&lt;/span&gt; part is directly available from the trained model, and only &lt;span class="math"&gt;\(\hat{\vartheta}_{mk}\)&lt;/span&gt; needs to be calculated regarding the new document);&lt;/li&gt;
&lt;li&gt;Repeat 2 until the Gibbs sampling converges;&lt;/li&gt;
&lt;li&gt;Calculate the topic distribution by &lt;span class="math"&gt;\(\hat{\vartheta}_{new,k}=\frac{n_{new}^{(k)}+\alpha_k}{\sum_{k=1}^K{(n_{new}^{(k)}+\alpha_k)}}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are multiple open-source LDA implementations available online. To learn how LDA could be implemented, a Python implementation can be found &lt;a href="https://github.com/nrolland/pyLDA/blob/master/src/pyLDA.py"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;LDA v.s. Probabilistic Latent Semantic Analysis (PLSA)&lt;/h3&gt;
&lt;p&gt;PLSA is a maximum likelihood (ML) model, while LDA is a maximum a posterior (MAP) model (Bayesian estimation). With that said, LDA would reduce to PLSA if a uniform Dirichlet prior is used. LDA is actually more complex than PLSA, so what could be the key advantages of LDA? The answer is the PRIOR! LDA would defeat PLSA, if there is a good prior for the data and the data by itself is not sufficient to train a model well. &lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;G. Heinrich, &lt;em&gt;&lt;a href="http://www.arbylon.net/publications/text-est.pdf"&gt;Parameter estimation for text analysis&lt;/a&gt;&lt;/em&gt;, 2005.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Sat, 26 Mar 2016 00:00:00 -0400</pubDate><guid>tag:stlong0521.github.io,2016-03-26:20160326 - LDA.html</guid><category>Natural Language Processing</category><category>Machine Learning</category></item><item><title>Hidden Markov Model and Part of Speech Tagging</title><link>https://stlong0521.github.io/20160319%20-%20HMM%20and%20POS.html</link><description>&lt;p&gt;In a Markov model, we generally assume that the states are directly observable or one state corresponds to one observation/event only. However, this is not always true. A good example would be: in speech recognition, we are supposed to identify a sequence of words given a sequence of utterances, in which case the states (words) are not directly observable and one single state (word) could have different observations (utterances). This is a perfect example that could be treated as a hidden Markov model (HMM), by which the hidden states can be inferred from the observations.&lt;/p&gt;
&lt;h3&gt;Elements of a Hidden Markov Model (HMM)&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;A hidden Markov model, &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt;, typically includes the following elements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time: &lt;span class="math"&gt;\(t=\{1,2,...,T\}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt; States: &lt;span class="math"&gt;\(Q=\{1,2,...,N\}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt; Observations: &lt;span class="math"&gt;\(O=\{1,2,...,M\}\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Initial Probabilities: &lt;span class="math"&gt;\(\pi_i=p(q_1=i),~1 \leq i \leq N\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Transition Probabilities: &lt;span class="math"&gt;\(a_{ij}=p(q_{t+1}=j|q_t=i),~1 \leq i,j \leq N\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Observation Probabilities: &lt;span class="math"&gt;\(b_j(k)=p(o_t=k|q_t=j)~1 \leq j \leq N, 1 \leq k \leq M\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The entire model can be characterized by &lt;span class="math"&gt;\(\Phi=(A,B,\pi)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(A=\{a_{ij}\}\)&lt;/span&gt;, &lt;span class="math"&gt;\(B=\{b_j(k)\}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi=\{\pi_i\}\)&lt;/span&gt;. The states are "hidden", since they are not directly observable, but reflected in observations with uncertainty.&lt;/p&gt;
&lt;h3&gt;Three Basic Problems for HMMs&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;There are three basic problems that are very important to real-world applications of HMMs:&lt;/p&gt;
&lt;h4&gt;Problem 1: Evaluation Problem&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given the observation sequence &lt;span class="math"&gt;\(O=o_1o_2...o_T\)&lt;/span&gt; and a model &lt;span class="math"&gt;\(\Phi=(A,B,\pi)\)&lt;/span&gt;, how to efficiently compute the probability of the observation sequence given the model, i.e., &lt;span class="math"&gt;\(p(O|\Phi)\)&lt;/span&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Let
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\alpha_t(i)=p(o_1o_2...o_t,q_t=i|\Phi)
\end{equation}&lt;/div&gt;
&lt;p&gt;
denote the probability that the state is &lt;span class="math"&gt;\(i\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; and we have a sequence of observations &lt;span class="math"&gt;\(o_1o_2...o_t\)&lt;/span&gt;. The evaluation problem can be solved by the forward algorithm as illustrated below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Base case: 
&lt;div class="math"&gt;\begin{equation}
\alpha_1(i)=p(o_1,q_1=i|\Phi)=p(o_1|q_1=i,\Phi)p(q_1=i|\Phi)=\pi_ib_i(o_1),~1 \leq i \leq N;
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Induction:
&lt;div class="math"&gt;\begin{equation}
\alpha_{t+1}(j)=\left[\sum_{i=1}^N{\alpha_{t}(i)a_{ij}}\right]b_j(o_{t+1}),~1 \leq j \leq N;
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Termination:
&lt;div class="math"&gt;\begin{equation}
p(O|\Phi)=\sum_{i=1}^N{\alpha_T(i)},
\end{equation}&lt;/div&gt;
&lt;div class="math"&gt;\begin{equation}
p(q_T=i|O,\Phi)=\frac{\alpha_T(i)}{\sum_{j=1}^N{\alpha_T(j)}}.
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm above essentially applies dynamic programming, and its complexity is &lt;span class="math"&gt;\(O(N^2T)\)&lt;/span&gt;.&lt;/p&gt;
&lt;h4&gt;Problem 2: Decoding Problem&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given the observation sequence &lt;span class="math"&gt;\(O=o_1o_2...o_T\)&lt;/span&gt; and a model &lt;span class="math"&gt;\(\Phi=(A,B,\pi)\)&lt;/span&gt;, how to choose the "best" state sequence &lt;span class="math"&gt;\(Q=q_1q_2...q_T\)&lt;/span&gt; (the most probable path) in terms of how good it explains the observations?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Define
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
v_t(i)=\max_{q_1q_2...q_{t-1}}{p(q_1q_2...q_{t-1},q_t=i,o_1o_2...o_t|\Phi)}
\end{equation}&lt;/div&gt;
&lt;p&gt;
as the best state sequence through which the state arrives at &lt;span class="math"&gt;\(i\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; with a sequence of observations &lt;span class="math"&gt;\(o_1o_2...o_t\)&lt;/span&gt;. The decoding problem can be solved by the Viterbi algorithm as illustrated below:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Base case: 
&lt;div class="math"&gt;\begin{equation}
v_1(i)=p(q_1=i,o_1|\Phi)=p(o_1|q_1=i,\Phi)p(q_1=i|\Phi)=\pi_ib_i(o_1),~1 \leq i \leq N;
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;Induction:
&lt;div class="math"&gt;\begin{equation}
v_{t+1}(j)=\left[\max_i{v_{t}(i)a_{ij}}\right]b_j(o_{t+1}),~1 \leq j \leq N,
\end{equation}&lt;/div&gt;
in which the optimal &lt;span class="math"&gt;\(i\)&lt;/span&gt; from the maximization should be stored properly for backtracking;&lt;/li&gt;
&lt;li&gt;Termination: The best state sequence can be determined by first finding the optimal final state
&lt;div class="math"&gt;\begin{equation}
q_T=\max_i{v_T(i)},
\end{equation}&lt;/div&gt;
and then backtracking all the way to the initial state.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The algorithm above also applies dynamic programming, and its complexity is &lt;span class="math"&gt;\(O(N^2T)\)&lt;/span&gt; as well.&lt;/p&gt;
&lt;h4&gt;Problem 3: Model Learning&lt;/h4&gt;
&lt;blockquote&gt;
&lt;p&gt;Given the observation sequence &lt;span class="math"&gt;\(O=o_1o_2...o_T\)&lt;/span&gt;, how to find the model &lt;span class="math"&gt;\(\Phi=(A,B,\pi)\)&lt;/span&gt; that maximizes &lt;span class="math"&gt;\(p(O|\Phi)\)&lt;/span&gt;?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A general maximum likelihood (ML) learning approach could determine the optimal &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\hat{\Phi}=\max_{\Phi}{p(O|\Phi)}.
\end{equation}&lt;/div&gt;
&lt;p&gt;
It is much easier to perform supervised learning, where the true state are tagged to each observation. Given &lt;span class="math"&gt;\(V\)&lt;/span&gt; training sequences in total, the model parameters can be estimated as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Supervised_Learning}
\hat{a}_{ij}=\frac{Count(q:i \rightarrow j)}{Count(q:i)},~~\hat{b}_j(k)=\frac{Count(q:j,o:k)}{Count(q:j)},~~\hat{\pi}_i=\frac{Count(q_1=i)}{V}.
\end{equation}&lt;/div&gt;
&lt;p&gt;It becomes a little bit tricky for unsupervised learning, where the true state are not tagged. To facilitate our model learning, we need to first introduce the following definition/calculation:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Episilon}
\varepsilon_t(i,j)=p(q_t=i,q_{t+1}=j|O,\Phi)=\frac{p(q_t=i,q_{t+1}=j,O|\Phi)}{p(O|\Phi)}=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum_{i=1}^N{\alpha_T(i)}},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(p(O|\Phi)\)&lt;/span&gt; is exactly Problem 1 we have yet talked about. &lt;span class="math"&gt;\(\beta_{t+1}(j)\)&lt;/span&gt; can be calculated using the backward algorithm, which is very similar to the forward algorithm in Problem 1 to calculate &lt;span class="math"&gt;\(\alpha_t(i)\)&lt;/span&gt; except the difference in the direction of calculation. Following (\ref{Eqn:Episilon}), we further introduce
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Gamma}
\gamma_t(i)=p(q_t=i|O,\Phi)=\sum_{j=1}^N{\varepsilon_t(i,j)}.
\end{equation}&lt;/div&gt;
&lt;p&gt;
Then the model parameters can be recomputed as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\hat{a}_{ij}&amp;amp;=\frac{Expected~number~of~transitions~from~state~i~to~j}{Expected~number~of~transitions~from~state~i}\\
&amp;amp;=\frac{\sum_{t=1}^{T-1}{\varepsilon_t(i,j)}}{\sum_{t=1}^{T-1}{\gamma_t(i)}},
\end{split}
\end{equation}&lt;/div&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\hat{b}_j(k)&amp;amp;=\frac{Expected~number~of~times~in~state~j~and~observing~k}{Expected~number~of~times~in~state~j}\\
&amp;amp;=\frac{\sum_{t=1,~s.t.~o_t=k}^{T}{\gamma_t(j)}}{\sum_{t=1}^{T}{\gamma_t(j)}},
\end{split}
\end{equation}&lt;/div&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\hat{\pi}_i&amp;amp;=Expected~number~of~times~in~state~i~at~time~t=1\\
&amp;amp;=\gamma_1(i).
\end{split}
\end{equation}&lt;/div&gt;
&lt;p&gt;Now we are ready to apply the &lt;a href="20160312 - EM and GMM.html"&gt;expectation maximization (EM) algorithm&lt;/a&gt; for HMM learning. More specifically:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the HMM, &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Repeat the two steps below until convergence:&lt;ul&gt;
&lt;li&gt;E Step: Given observations &lt;span class="math"&gt;\(o_1o_2...o_T\)&lt;/span&gt; and the model &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt;, compute &lt;span class="math"&gt;\(\varepsilon_t(i,j)\)&lt;/span&gt; by (\ref{Eqn:Episilon}) and &lt;span class="math"&gt;\(\gamma_t(i)\)&lt;/span&gt; by (\ref{Eqn:Gamma});&lt;/li&gt;
&lt;li&gt;M Step: Update the model &lt;span class="math"&gt;\(\Phi\)&lt;/span&gt; by recomputing parameters using the three equations right above.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Part of Speech (POS) Tagging&lt;/h3&gt;
&lt;p&gt;In natural language processing, part of speech (POS) tagging is to associate with each word in a sentence a lexical tag. As an example, Janet (NNP) will (MD) back (VB) the (DT) bill (NN), in which each POS tag describes what its corresponding word is about. In this particular example, "VB" tells that "back" is a verb, and "NN" tells that "bill" is a noun, etc.&lt;/p&gt;
&lt;p&gt;POS tagging is very useful, because it is usually the first step of many practical tasks, e.g., speech synthesis, grammatical parsing and information extraction. For instance, if we want to pronounce the word "record" correctly, we need to first learn from context if it is a noun or verb and then determine where the stress is in its pronunciation. A similar argument applies to grammatical parsing and information extraction as well.&lt;/p&gt;
&lt;p&gt;We need to do some preprocessing before performing POS tagging using HMM. First, because the vocabulary size could be very large while most of the words are not frequently used, we replace each low-frequency word with a special word "UNKA". This is very helpful to reduce the vocabulary size, and thus reduce the memory cost on storing the probability matrix. Second, for each sentence, we add two tags to represent sentence boundaries, e.g., "START" and "END".&lt;/p&gt;
&lt;p&gt;Now we are ready to apply HMM to perform POS tagging. The model can be characterized by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Time: length of each sentence;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(N\)&lt;/span&gt; States: POS tags, e.g., 45 POS tags from Penn Treebank;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(M\)&lt;/span&gt; Observations: vocabulary (compressed by replacing low-frequency words with "UNKA");&lt;/li&gt;
&lt;li&gt;Initial Probabilities: probability of each tag associated to the first word;&lt;/li&gt;
&lt;li&gt;Transition Probabilities: &lt;span class="math"&gt;\(p(t_{i+1}|t_i)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; represents the tag for the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th word;&lt;/li&gt;
&lt;li&gt;Observation Probabilities: &lt;span class="math"&gt;\(p(w|t)\)&lt;/span&gt;, where &lt;span class="math"&gt;\(t\)&lt;/span&gt; stands for a tag and &lt;span class="math"&gt;\(w\)&lt;/span&gt; stands for a word.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Once we finish training the model, e.g., under supervised learning by (\ref{Eqn:Supervised_Learning}), we will then be able to tag new sentences applying the Viterbi algorithm as previously illustrated in Problem 2 for HMM. To see details about implementing POS tagging using HMM, &lt;a href="https://github.com/stlong0521/hmm-pos"&gt;click here&lt;/a&gt; for demo codes.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;L. R. Rabiner, &lt;em&gt;A tutorial on hidden Markov models and selected applications in speech recognition&lt;/em&gt;, in Proceedings of the IEEE, vol. 77, no. 2, pp. 257-286, Feb 1989.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Sat, 19 Mar 2016 00:00:00 -0400</pubDate><guid>tag:stlong0521.github.io,2016-03-19:20160319 - HMM and POS.html</guid><category>Natural Language Processing</category><category>Machine Learning</category></item><item><title>Expectation Maximization Algorithm and Gaussian Mixture Model</title><link>https://stlong0521.github.io/20160312%20-%20EM%20and%20GMM.html</link><description>&lt;p&gt;In statistical modeling, it is possible that some observations are just missing. For example, when flipping two biased coins with unknown biases, we only have a sequence of observations on heads and tails, but forgot to record which coin each observation comes from. In this case, the conventional maximum likelihood (ML) or maximum a posteriori (MAP) algorithm would no longer be able to work, and it is time for the expectation maximization (EM) algorithm to come into play.&lt;/p&gt;
&lt;h3&gt;A Motivating Example&lt;/h3&gt;
&lt;p&gt;Although the two-biased-coin example above works as a valid example, another example will be discussed here, as it is more relevant to practical needs. Let us assume that we have a collection of float numbers, which come from two different Gaussian distributions. Unfortunately, we do not know which distribution each number comes from. Now we are supposed to learn the two Gaussian distributions (i,e, their means and variances) from the given data. This is the well-known Gaussian mixture model (GMM). What makes things difficult is that we have missing observations, i.e., membership of each number towards the two distributions. Though conventional ML or MAP would not work here, this is a perfect problem that EM can handle.&lt;/p&gt;
&lt;h3&gt;Expectation Maximization (EM) Algorithm&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Let us consider a statistical model with a vector of unknown parameters &lt;span class="math"&gt;\(\boldsymbol\theta\)&lt;/span&gt;, which generates a set of observed data &lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt; and a set of missing observations &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt;. The likelihood function, &lt;span class="math"&gt;\(p(\textbf{X},\textbf{Z}|\boldsymbol\theta)\)&lt;/span&gt;, characterizes the probability that &lt;span class="math"&gt;\(\textbf{X}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt; appear given the model with parameters &lt;span class="math"&gt;\(\boldsymbol\theta\)&lt;/span&gt;. An intuitive idea to estimate &lt;span class="math"&gt;\(\boldsymbol\theta\)&lt;/span&gt; would be trying to perform the maximum likelihood estimation (MLE) considering all possible &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt;, i.e.,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\max_{\boldsymbol\theta}{\ln{p(\textbf{X}|\boldsymbol\theta)}}=\max_{\boldsymbol\theta}{\ln{\sum_{\textbf{Z}}{p(\textbf{X},\textbf{Z}|\boldsymbol\theta)}}}=\max_{\boldsymbol\theta}{\ln{\sum_{\textbf{Z}}{p(\textbf{X}|\textbf{Z},\boldsymbol\theta)p(\textbf{Z}|\boldsymbol\theta)}}}.
\end{equation}&lt;/div&gt;
&lt;p&gt;
Unfortunately, the problem above is not directly tractable, since we do not have any prior knowledge on the missing observations &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The EM algorithm aims to solve the problem above by starting with a guess on &lt;span class="math"&gt;\(\boldsymbol\theta=\boldsymbol\theta_{0}\)&lt;/span&gt; and then iteratively applying the two steps as indicated below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Expectation Step (E Step):&lt;/em&gt; Calculate the log likelihood with respect to &lt;span class="math"&gt;\(\boldsymbol\theta\)&lt;/span&gt; given &lt;span class="math"&gt;\(\boldsymbol\theta_{t}\)&lt;/span&gt; by
&lt;div class="math"&gt;\begin{equation}
\mathcal{L}(\boldsymbol\theta|\boldsymbol\theta_{t})=\ln{\sum_{\textbf{Z}}{p(\textbf{X}|\textbf{Z},\boldsymbol\theta_{t})p(\textbf{Z}|\boldsymbol\theta_{t})}};
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Maximization Step (M Step):&lt;/em&gt; Find the parameter vector that maximizes the log likelihood above and then update it as
&lt;div class="math"&gt;\begin{equation}
\theta_{t+1}={\arg \, \max}_{\theta}{\mathcal{L}(\boldsymbol\theta|\boldsymbol\theta_{t})}.
\end{equation}&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two things that should be noted here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are two categories of EM: &lt;em&gt;hard&lt;/em&gt; EM and &lt;em&gt;soft&lt;/em&gt; EM. The algorithm illustrated above is soft EM, because the log likelihood in the E step is weighted upon all possible &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt; with their probabilities. While in hard EM, instead of using weighted average, we simply select the most probable &lt;span class="math"&gt;\(\textbf{Z}\)&lt;/span&gt; and then move forward. The &lt;a href="https://en.wikipedia.org/wiki/K-means_clustering"&gt;k-means algorithm&lt;/a&gt; is a good example of hard EM algorithm.&lt;/li&gt;
&lt;li&gt;The EM algorithm typically converges to a local optimum, and &lt;em&gt;cannot&lt;/em&gt; guarantee global optimum. With this being said, the solution might differ with different initialization, and it is possibly helpful to try more than one initialization when applying EM practically.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Gaussian Mixture Model (GMM)&lt;/h3&gt;
&lt;p&gt;In the motivating example, a GMM with two Gaussian distributions was introduced. Here we are going to extend it to a general case with &lt;span class="math"&gt;\(K\)&lt;/span&gt; Gaussian distributions, and the data points will be generalized to be multidimensional. At the same time, we will discuss how it can be used for clustering.&lt;/p&gt;
&lt;p&gt;Given a data set containing &lt;span class="math"&gt;\(N\)&lt;/span&gt; data points, &lt;span class="math"&gt;\(\mathcal{D}=\{\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_N\}\)&lt;/span&gt;, in which each data point is a &lt;span class="math"&gt;\(M\)&lt;/span&gt;-dimensional column vector and comes from one of &lt;span class="math"&gt;\(K\)&lt;/span&gt; Gaussian distributions. Here we will introduce &lt;span class="math"&gt;\(\mathcal{Z}=\{z_1,z_2,...,z_N\}\)&lt;/span&gt; with &lt;span class="math"&gt;\(z_i\in\{1,2,...,K\}\)&lt;/span&gt; as latent (hidden) variables to represent the cluster membership of the data points in &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt;. The &lt;span class="math"&gt;\(K\)&lt;/span&gt; Gaussian distributions are characterized by &lt;span class="math"&gt;\(\mathcal{N}(\boldsymbol\mu_j,\boldsymbol\Sigma_j)\)&lt;/span&gt; for &lt;span class="math"&gt;\(j=1,2,...,K\)&lt;/span&gt;, and the &lt;span class="math"&gt;\(j\)&lt;/span&gt;th distribution has a weight of &lt;span class="math"&gt;\(\pi_j\)&lt;/span&gt; accounted in the overall distribution. Let us first try to map this GMM model to the EM algorithm component by component:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; is the observed data;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathcal{Z}\)&lt;/span&gt; is the missing observations;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\boldsymbol\mu_j\)&lt;/span&gt;, &lt;span class="math"&gt;\(\boldsymbol\Sigma_j\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi_j\)&lt;/span&gt; are the unknown model parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Following the EM algorithm, we will start with a guess on the unknown parameters, and then iteratively applying E step and M step until convergence. In the E step, we calculate the log likelihood based on given model parameters by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
LL&amp;amp;=\ln{p(\textbf{x}_1,\textbf{x}_2,...,\textbf{x}_N)}\\
&amp;amp;=\ln{\prod_{i=1}^{N}{p(\textbf{x}_i)}}\\
&amp;amp;=\ln{\prod_{i=1}^{N}{\sum_{j=1}^{K}{p(z_i=j)p(\textbf{x}_i|z_i=j)}}}\\
&amp;amp;=\sum_{i=1}^{N}{\ln\left(\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}\right)}.
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;In the M step, we maximize the log likelihood by solving the optimization problem below:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
\max_{\boldsymbol\mu_j,\boldsymbol\Sigma_j,\pi_j}~~&amp;amp;{LL}\\
&amp;amp;s.t.~\sum_{j=1}^{K}{\pi_j}=1.
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;
We can apply Lagrange multiplier to solve the problem above. Let
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
L=\sum_{i=1}^{N}{\ln\left(\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}\right)}-\lambda\left(\sum_{j=1}^{K}{\pi_j}-1\right),
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is the Lagrange multiplier. Taking partial derivatives and setting them to zero, we can obtain the optimal parameters as below:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Maximization_1}
\boldsymbol\mu_j=\frac{\sum_{i=1}^{N}{\gamma_{ij}}\textbf{x}_i}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}&lt;/div&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Maximization_2}
\boldsymbol\Sigma_j=\frac{\sum_{i=1}^{N}{\gamma_{ij}}(\textbf{x}_i-\boldsymbol\mu_j)(\textbf{x}_i-\boldsymbol\mu_j)^T}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}&lt;/div&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Maximization_3}
\pi_j=\frac{1}{N}{\sum_{i=1}^{N}{\gamma_{ij}}},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\gamma_{ij}=p(z_i=j|\textbf{x}_i)\)&lt;/span&gt; is the cluster membership, which can be calculated using Bayes theorem,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\begin{split}
\gamma_{ij}&amp;amp;=p(z_i=j|\textbf{x}_i)\\
&amp;amp;=\frac{p(z_i=j)p(\textbf{x}_i|z_i=j)}{\sum_{j=1}^{K}{p(z_i=j)p(\textbf{x}_i|z_i=j)}}\\
&amp;amp;=\frac{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}{\sum_{j=1}^{K}{\pi_j\mathcal{N}(\textbf{x}_i|\boldsymbol\mu_j,\boldsymbol\Sigma_j)}}.
\end{split}
\end{equation}&lt;/div&gt;
&lt;p&gt;To summarize, the GMM model can be learned using EM algorithm as in the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize &lt;span class="math"&gt;\(\boldsymbol\mu_j\)&lt;/span&gt;, &lt;span class="math"&gt;\(\boldsymbol\Sigma_j\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi_j\)&lt;/span&gt; for &lt;span class="math"&gt;\(j=1,2,...,K\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Repeat the following two steps until the log likelihood converges:&lt;ul&gt;
&lt;li&gt;E Step: Estimate cluster membership &lt;span class="math"&gt;\(\gamma_{ij}\)&lt;/span&gt; by the equation right above for all data point &lt;span class="math"&gt;\(x_i\)&lt;/span&gt; and cluster &lt;span class="math"&gt;\(z_i=j\)&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;M Step: Maximize the log likelihood and update the model parameters by (\ref{Eqn:Maximization_1})-(\ref{Eqn:Maximization_3}) based on cluster membership &lt;span class="math"&gt;\(\gamma_{ij}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Wikipedia, &lt;a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"&gt;&lt;em&gt;Expectation–maximization algorithm&lt;/em&gt;&lt;/a&gt;, accessed on Mar 12, 2016.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Sat, 12 Mar 2016 00:00:00 -0500</pubDate><guid>tag:stlong0521.github.io,2016-03-12:20160312 - EM and GMM.html</guid><category>Data Science</category><category>Machine Learning</category></item><item><title>Locating and Filling Missing Words in Sentences</title><link>https://stlong0521.github.io/20160305%20-%20Missing%20Word.html</link><description>&lt;p&gt;There has been many occasions that we have incomplete sentences that are needed to completed. One example is that in speech recognition noisy environment can lead to unrecognizable words, but we still hope to recover and understand the complete sentence (e.g., by inference); another example is sentence completion questions that appear in language tests (e.g., SAT, GRE, etc.).&lt;/p&gt;
&lt;h3&gt;What are Exactly the Problem?&lt;/h3&gt;
&lt;p&gt;Generally, the problem we are aiming to solve is locating and filling any missing words in incomplete sentences. However, this problem seems too ambitious so far, and we direct ourselves to a simplified version of this problem. To simplify the problem, we assume that there is only one missing word in a sentence, and the missing word is neither the first word nor the last word of the sentence. This problem originally comes from &lt;a href="https://www.kaggle.com/c/billion-word-imputation"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Locating the Missing Word&lt;/h3&gt;
&lt;p&gt;Two approaches are presented here so as to locate the missing word.&lt;/p&gt;
&lt;h4&gt;N-gram Model&lt;/h4&gt;
&lt;p&gt;For a given training data set, define &lt;span class="math"&gt;\(C(w_1,w_2)\)&lt;/span&gt; as the number of occurrences of the bigram pattern &lt;span class="math"&gt;\((w_1,w_2)\)&lt;/span&gt;, and &lt;span class="math"&gt;\(C(w_1,w,w_2)\)&lt;/span&gt; the number of occurrences of the trigram pattern &lt;span class="math"&gt;\((w_1,w,w_2)\)&lt;/span&gt;. Then, the number of occurrences of the pattern, where there is one and only one word between &lt;span class="math"&gt;\(w_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;, can be calculated by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
D(w_1,w_2)=\sum_{w\in{V}}C(w_1,w,w_2),
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(V\)&lt;/span&gt; is the vocabulary.&lt;/p&gt;
&lt;p&gt;Consider a particular location, &lt;span class="math"&gt;\(l\)&lt;/span&gt;, of an incomplete sentence of length &lt;span class="math"&gt;\(L\)&lt;/span&gt;, and let &lt;span class="math"&gt;\(w_l\)&lt;/span&gt; be the &lt;span class="math"&gt;\(l\)&lt;/span&gt;th word in the sentence. &lt;span class="math"&gt;\(D(w_{l-1},w_{l})\)&lt;/span&gt; would be the number of positive votes from the training data set for missing word at this location, while &lt;span class="math"&gt;\(C(w_{l-1},w_{l})\)&lt;/span&gt; would be correspondingly the number of negative votes. We define the score indicating there is a missing word at location &lt;span class="math"&gt;\(l\)&lt;/span&gt; as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Score}
S_l=\frac{D(w_{l-1},w_{l})^{1+\gamma}}{C(w_{l-1},w_{l})+D(w_{l-1},w_{l})}-\frac{C(w_{l-1},w_{l})^{1+\gamma}}{C(w_{l-1},w_{l})+D(w_{l-1},w_{l})},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; is a small positive constant. Hence, the missing word location can be identified by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\hat{l}={\arg \, \max}_{1 \leq l \leq L-1} S_l.
\end{equation}&lt;/div&gt;
&lt;p&gt;Note that in (\ref{Eqn:Score}), if we set &lt;span class="math"&gt;\(\gamma=0\)&lt;/span&gt;, the left part would be exactly the percentage of positive votes for missing word at that location, and the right part is the percentage of negative votes. It seems a fairly reasonable score, then &lt;em&gt;why do we still need a positive &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;&lt;/em&gt;? The underlying reason is that intuitively the more number of votes for a particular decision, the more confident we are on that decision. This trend is reflected in a positive &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, which can be viewed as &lt;em&gt;sparse vote penalty&lt;/em&gt; and is useful in breaking ties in the missing word location voting. That is, if we have exactly the same ratio of positive votes relative to negative votes for two candidate locations, e.g., 80 positive votes v.s. 20 negative votes for location A, and 8 positive votes v.s. 2 negative votes for location B, we would believe that location A is more likely to be the missing word location compared with location B.&lt;/p&gt;
&lt;h4&gt;Word Distance Statistics (WDS)&lt;/h4&gt;
&lt;p&gt;In view of the fact that the statistics of the two words immediately adjacent to a given location contribute a lot in deciding whether the location has a word missing, we tentatively guess that all the words within a window centered at that location would more or less contribute some information as well. As a result, we introduce the concept of word distance statistics (WDS).&lt;/p&gt;
&lt;p&gt;More specifically, we use &lt;span class="math"&gt;\(\widetilde{C}(w_1,w_2,m)\)&lt;/span&gt; to denote the number of occurrences of the pattern, where there is exactly &lt;span class="math"&gt;\(m\)&lt;/span&gt; words between &lt;span class="math"&gt;\(w_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(w_2\)&lt;/span&gt;, i.e., the word distance of &lt;span class="math"&gt;\(w_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(w_2\)&lt;/span&gt; is &lt;span class="math"&gt;\(m\)&lt;/span&gt;. For a given location &lt;span class="math"&gt;\(l\)&lt;/span&gt; in an incomplete sentence and a word window size &lt;span class="math"&gt;\(W\)&lt;/span&gt;, we are interested in the word distance statistics of each word pair, in which one word &lt;span class="math"&gt;\(w_i\)&lt;/span&gt; is on the left of the location &lt;span class="math"&gt;\(l\)&lt;/span&gt;, and the other word &lt;span class="math"&gt;\(w_j\)&lt;/span&gt; is on the right, as illustrated in Fig. 1.&lt;/p&gt;
&lt;figure align="center"&gt;
&lt;img src="/figures/20160305/WDS.png" alt="WDS"&gt;
&lt;figcaption align="center"&gt;Fig. 1: Word distance illustration.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Formally, for any &lt;span class="math"&gt;\(l-W/2 \leq i \leq l-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(l \leq j \leq l+W/2-1\)&lt;/span&gt;, &lt;span class="math"&gt;\(\widetilde{C}(w_i,w_j,j-i)\)&lt;/span&gt; would be the number of positive votes for missing word at this location, while &lt;span class="math"&gt;\(\widetilde{C}(w_i,w_j,j-i-1)\)&lt;/span&gt; is the number of negative votes. Applying the idea in (\ref{Eqn:Score}), for each word pair &lt;span class="math"&gt;\((w_i,w_j)\)&lt;/span&gt;, we extract its feature as the score indicating there is a missing word at location &lt;span class="math"&gt;\(l\)&lt;/span&gt;, i.e.,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:ScoreGeneralized}
S_l(i,j)=\frac{\widetilde{C}(w_i,w_j,j-i)^{1+\gamma}}{\widetilde{C}(w_i,w_j,j-i)+\widetilde{C}(w_i,w_j,j-i-1)}-\frac{\widetilde{C}(w_i,w_j,j-i-1)^{1+\gamma}}{\widetilde{C}(w_i,w_j,j-i)+\widetilde{C}(w_i,w_j,j-i-1)}.
\end{equation}&lt;/div&gt;
&lt;p&gt;
As a special example, let &lt;span class="math"&gt;\(i=l-1\)&lt;/span&gt; and &lt;span class="math"&gt;\(j=l\)&lt;/span&gt;, (\ref{Eqn:ScoreGeneralized}) would be reduced to (\ref{Eqn:Score}).&lt;/p&gt;
&lt;p&gt;To find the missing word location, we need to assign different weights to the extracted features, &lt;span class="math"&gt;\(S_l(i,j)\)&lt;/span&gt;. Then, the missing word location can be determined by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:LocationDetermination}
\hat{l}={\arg \, \max}_{1 \leq l \leq L-1} \sum_{l-\frac{W}{2} \leq i \leq l-1}\sum_{l \leq j \leq l+\frac{W}{2}-1}v(i,j)S_l(i,j),
\end{equation}&lt;/div&gt;
&lt;p&gt;
where the weight, &lt;span class="math"&gt;\(v(i,j)\)&lt;/span&gt;, should be monotonically decreasing with respect to &lt;span class="math"&gt;\(|j-i|\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Filling the Missing Word&lt;/h3&gt;
&lt;p&gt;To find the most probable word in the given missing word location, we take into account five conditional probabilities, as shown in Table 1, to explore the statistical connection between the candidate words and the surrounding words at the missing word location. Ultimately, the most probable missing word can be determined by
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\hat{w}={\arg \, \max}_{w\in{B}} \sum_{1 \leq i \leq 5} v_iP_i,
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(B\)&lt;/span&gt; is the candidate word space (detailed &lt;a href="https://github.com/stlong0521/missing-word/blob/master/Project%20Report.pdf"&gt;here&lt;/a&gt;), and the weight &lt;span class="math"&gt;\(v_i\)&lt;/span&gt; is used to reflect the importance of each conditional probability in contributing to the final score.&lt;/p&gt;
&lt;figure align="center"&gt;
&lt;img src="/figures/20160305/CondProb.png" alt="CondProb"&gt;
&lt;figcaption align="center"&gt;Table 1: Conditional probabilities considered in missing word filling, in which "*" denotes an arbitrary word.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3&gt;Experimental Results&lt;/h3&gt;
&lt;p&gt;The training data contains &lt;span class="math"&gt;\(30,301,028\)&lt;/span&gt; complete sentences, of which the average sentence length is approximately &lt;span class="math"&gt;\(25\)&lt;/span&gt;. In the vocabulary with a size of &lt;span class="math"&gt;\(2,425,337\)&lt;/span&gt;, &lt;span class="math"&gt;\(14,216\)&lt;/span&gt; words that have occurred in at least &lt;span class="math"&gt;\(0.1\%\)&lt;/span&gt; of total sentences are labeled as high-frequency words, and the remaining &lt;span class="math"&gt;\(58,417,315\)&lt;/span&gt; words are labeled as 'UNKA'. To perform the cross validation, in our experiments, the training data is splitted into two part, TRAIN and DEV. The TRAIN set is used to train our models, and the DEV set is applied to test our models.&lt;/p&gt;
&lt;h4&gt;Missing Word Location&lt;/h4&gt;
&lt;p&gt;Table 2 shows the estimation accuracy of the missing word locations for the two proposed approaches, N-gram and WDS. For comparison, we list the corresponding probabilities by chance as well. Each entry shows the probabilities that the correct location is included in the ranked candidate location list returned by each approach, where the list size varies from &lt;span class="math"&gt;\(1\)&lt;/span&gt; to &lt;span class="math"&gt;\(10\)&lt;/span&gt;. The sparse vote penalty coefficient, &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;, is set to 0.01. In the WDS approach, we consider a word window size &lt;span class="math"&gt;\(W=4\)&lt;/span&gt;, i.e., four pairs of words are taken into account.&lt;/p&gt;
&lt;table class="table table-striped table-bordered table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;Top 1&lt;/th&gt;
&lt;th align="center"&gt;Top 2&lt;/th&gt;
&lt;th align="center"&gt;Top 3&lt;/th&gt;
&lt;th align="center"&gt;Top 5&lt;/th&gt;
&lt;th align="center"&gt;Top 10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Chance&lt;/td&gt;
&lt;td align="center"&gt;4%&lt;/td&gt;
&lt;td align="center"&gt;8%&lt;/td&gt;
&lt;td align="center"&gt;12%&lt;/td&gt;
&lt;td align="center"&gt;20%&lt;/td&gt;
&lt;td align="center"&gt;40%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;N-gram&lt;/td&gt;
&lt;td align="center"&gt;51.47%&lt;/td&gt;
&lt;td align="center"&gt;63.70%&lt;/td&gt;
&lt;td align="center"&gt;71.00%&lt;/td&gt;
&lt;td align="center"&gt;80.26%&lt;/td&gt;
&lt;td align="center"&gt;91.54%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align="center"&gt;WDS&lt;/td&gt;
&lt;td align="center"&gt;52.06%&lt;/td&gt;
&lt;td align="center"&gt;64.50%&lt;/td&gt;
&lt;td align="center"&gt;71.76%&lt;/td&gt;
&lt;td align="center"&gt;80.91%&lt;/td&gt;
&lt;td align="center"&gt;91.93%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;figcaption align="center"&gt;Table 2: Accuracy of missing word location.&lt;/figcaption&gt;

&lt;h4&gt;Missing Word Filling&lt;/h4&gt;
&lt;p&gt;Table 3 shows the accuracies of filling the missing word given the location. Each row of the second column shows the probability that the correct word is included in the ranked candidate words list returned by the proposed approach.&lt;/p&gt;
&lt;table class="table table-striped table-bordered table-hover"&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align="center"&gt;&lt;/th&gt;
&lt;th align="center"&gt;Top 1&lt;/th&gt;
&lt;th align="center"&gt;Top 2&lt;/th&gt;
&lt;th align="center"&gt;Top 3&lt;/th&gt;
&lt;th align="center"&gt;Top 5&lt;/th&gt;
&lt;th align="center"&gt;Top 10&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align="center"&gt;Accuracy&lt;/td&gt;
&lt;td align="center"&gt;32.15%&lt;/td&gt;
&lt;td align="center"&gt;41.49%&lt;/td&gt;
&lt;td align="center"&gt;46.23%&lt;/td&gt;
&lt;td align="center"&gt;52.02%&lt;/td&gt;
&lt;td align="center"&gt;59.15%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;figcaption align="center"&gt;Table 3: Accuracy of missing word filling.&lt;/figcaption&gt;

&lt;h3&gt;Acknowledgement&lt;/h3&gt;
&lt;p&gt;I did this project with my partner, &lt;a href="https://zhwa.github.io/"&gt;Zhe Wang&lt;/a&gt;. To see the codes and/or report, &lt;a href="https://github.com/stlong0521/missing-word"&gt;click here&lt;/a&gt; for more information.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Sat, 05 Mar 2016 00:00:00 -0500</pubDate><guid>tag:stlong0521.github.io,2016-03-05:20160305 - Missing Word.html</guid><category>Natural Language Processing</category></item><item><title>Binary and Multiclass Logistic Regression Classifiers</title><link>https://stlong0521.github.io/20160228%20-%20Logistic%20Regression.html</link><description>&lt;p&gt;The generative classification model, such as Naive Bayes, tries to learn the probabilities and then predict by using Bayes rules to calculate the posterior, &lt;span class="math"&gt;\(p(y|\textbf{x})\)&lt;/span&gt;. However, discrimitive classifiers model the posterior directly. As one of the most popular discrimitive classifiers, logistic regression directly models the linear decision boundary.&lt;/p&gt;
&lt;h3&gt;Binary Logistic Regression Classifier&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Let us start with the binary case. For an M-dimensional feature vector &lt;span class="math"&gt;\(\textbf{x}=[x_1,x_2,...,x_M]^T\)&lt;/span&gt;, the posterior probability of class &lt;span class="math"&gt;\(y\in\{\pm{1}\}\)&lt;/span&gt; given &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; is assumed to satisfy
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\ln{\frac{p(y=1|\textbf{x})}{p(y=-1|\textbf{x})}}=\textbf{w}^T\textbf{x},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\textbf{w}=[w_1,w_2,...,w_M]^T\)&lt;/span&gt; is the weighting vector to be learned. Given the constraint that &lt;span class="math"&gt;\(p(y=1|\textbf{x})+p(y=-1|\textbf{x})=1\)&lt;/span&gt;, it follows that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Prob_Binary}
p(y|\textbf{x})=\frac{1}{1+\exp(-y\textbf{w}^T\textbf{x})}=\sigma(y\textbf{w}^T\textbf{x}),
\end{equation}&lt;/div&gt;
&lt;p&gt;
in which we can observe the logistic sigmoid function &lt;span class="math"&gt;\(\sigma(a)=\frac{1}{1+\exp(-a)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Based on the assumptions above, the weighting vector, &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt;, can be learned by maximum likelihood estimation (MLE). More specifically, given training data set &lt;span class="math"&gt;\(\mathcal{D}=\{(\textbf{x}_1,y_1),(\textbf{x}_2,y_2),...,(\textbf{x}_N,y_N)\}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
\textbf{w}^*&amp;amp;=\max_{\textbf{w}}{\mathcal{L}(\textbf{w})}\\
&amp;amp;=\max_{\textbf{w}}{\sum_{i=1}^N\ln{{p(y_i|\textbf{x}_i)}}}\\
&amp;amp;=\max_{\textbf{w}}{\sum_{i=1}^N{\ln{\frac{1}{1+\exp(-y_i\textbf{w}^T\textbf{x}_i)}}}}\\
&amp;amp;=\min_{\textbf{w}}{\sum_{i=1}^N{\ln{(1+\exp(-y_i\textbf{w}^T\textbf{x}_i))}}}.
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;
We have a convex objective function here, and we can calculate the optimal solution by applying gradient descent. The gradient can be drawn as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
\nabla{\mathcal{L}(\textbf{w})}&amp;amp;=\sum_{i=1}^N{\frac{-y_i\textbf{x}_i\exp(-y_i\textbf{w}^T\textbf{x}_i)}{1+\exp(-y_i\textbf{w}^T\textbf{x}_i)}}\\
&amp;amp;=-\sum_{i=1}^N{y_i\textbf{x}_i(1-p(y_i|\textbf{x}_i))}.
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;
Then, we can learn the optimal &lt;span class="math"&gt;\(\textbf{w}\)&lt;/span&gt; by starting with an initial &lt;span class="math"&gt;\(\textbf{w}_0\)&lt;/span&gt; and iterating as follows:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Iteration_Binary}
\textbf{w}_{t+1}=\textbf{w}_{t}-\eta_t\nabla{\mathcal{L}(\textbf{w})},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\eta_t\)&lt;/span&gt; is the learning step size. It can be invariant to time, but time-varying step sizes could potential reduce the convergence time, e.g., setting &lt;span class="math"&gt;\(\eta_t\propto{1/\sqrt{t}}\)&lt;/span&gt; such that the step size decreases with an increasing time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;Multiclass Logistic Regression Classifier&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;When it is generalized to multiclass case, the logistic regression model needs to adapt accordingly. Now we have &lt;span class="math"&gt;\(K\)&lt;/span&gt; possible classes, that is, &lt;span class="math"&gt;\(y\in\{1,2,..,K\}\)&lt;/span&gt;. It is assumed that the posterior probability of class &lt;span class="math"&gt;\(y=k\)&lt;/span&gt; given &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt; follows
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\ln{p(y=k|\textbf{x})}\propto\textbf{w}_k^T\textbf{x},
\end{equation}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\textbf{w}_k\)&lt;/span&gt; is a column weighting vector corresponding to class &lt;span class="math"&gt;\(k\)&lt;/span&gt;. Considering all classes &lt;span class="math"&gt;\(k=1,2,...,K\)&lt;/span&gt;, we would have a weighting matrix that includes all &lt;span class="math"&gt;\(K\)&lt;/span&gt; weighting vectors. That is, &lt;span class="math"&gt;\(\textbf{W}=[\textbf{w}_1,\textbf{w}_2,...,\textbf{w}_K]\)&lt;/span&gt;.
Under the constraint
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
\sum_{k=1}^K{p(y=k|\textbf{x})}=1,
\end{equation}&lt;/div&gt;
&lt;p&gt;
it then follows that
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation} \label{Eqn:Prob_Multiple}
p(y=k|\textbf{x})=\frac{\exp(\textbf{w}_k^T\textbf{x})}{\sum_{j=1}^K{\exp(\textbf{w}_j^T\textbf{x})}}.
\end{equation}&lt;/div&gt;
&lt;p&gt;The weighting matrix, &lt;span class="math"&gt;\(\textbf{W}\)&lt;/span&gt;, can be similarly learned by maximum likelihood estimation (MLE). More specifically, given training data set &lt;span class="math"&gt;\(\mathcal{D}=\{(\textbf{x}_1,y_1),(\textbf{x}_2,y_2),...(\textbf{x}_N,y_N)\}\)&lt;/span&gt;,
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
\textbf{W}^*&amp;amp;=\max_{\textbf{W}}{\mathcal{L}(\textbf{W})}\\
&amp;amp;=\max_{\textbf{W}}{\sum_{i=1}^N\ln{{p(y_i|\textbf{x}_i)}}}\\
&amp;amp;=\max_{\textbf{W}}{\sum_{i=1}^N{\ln{\frac{\exp(\textbf{w}_{y_i}^T\textbf{x})}{\sum_{j=1}^K{\exp(\textbf{w}_j^T\textbf{x})}}}}}.
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;
The gradient of the objective function with respect to each &lt;span class="math"&gt;\(\textbf{w}_k\)&lt;/span&gt; can be calculated as
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
\begin{aligned}
\frac{\partial{\mathcal{L}(\textbf{W})}}{\partial{\textbf{w}_k}}&amp;amp;=\sum_{i=1}^N{\textbf{x}_i\left(I(y_i=k)-\frac{\exp(\textbf{w}_k^T\textbf{x})}{\sum_{j=1}^K{\exp(\textbf{w}_j^T\textbf{x})}}\right)}\\
&amp;amp;=\sum_{i=1}^N{\textbf{x}_i(I(y_i=k)-p(y_i=k|\textbf{x}_i))},
\end{aligned}
\end{align}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(I(\cdot)\)&lt;/span&gt; is a binary indicator function. Applying gradient descent, the optimal solution can be obtained by iterating as follows:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}\label{Eqn:Iteration_Multiple}
\textbf{w}_{k,t+1}=\textbf{w}_{k,t}+\eta_{t}\frac{\partial{\mathcal{L}(\textbf{W})}}{\partial{\textbf{w}_k}}.
\end{equation}&lt;/div&gt;
&lt;p&gt;
Note that we have "&lt;span class="math"&gt;\(+\)&lt;/span&gt;" in (\ref{Eqn:Iteration_Multiple}) instead of "&lt;span class="math"&gt;\(-\)&lt;/span&gt;" in (\ref{Eqn:Iteration_Binary}), because the maximum likelihood estimation in the binary case is eventually converted to a minimization problem, while here we keep performing maximization.&lt;/p&gt;
&lt;h3&gt;How to Perform Predictions?&lt;/h3&gt;
&lt;p&gt;Once the optimal weights are learned from the logistic regression model, for any new feature vector &lt;span class="math"&gt;\(\textbf{x}\)&lt;/span&gt;, we can easily calculate the probability that it is associated to each class label &lt;span class="math"&gt;\(k\)&lt;/span&gt; by (\ref{Eqn:Prob_Binary}) in the binary case or (\ref{Eqn:Prob_Multiple}) in the multiclass case. With the probabilities for each class label available, we can then perform:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a hard decision by identifying the class label with the highest probability, or&lt;/li&gt;
&lt;li&gt;a soft decision by showing the top &lt;span class="math"&gt;\(k\)&lt;/span&gt; most probable class labels with their corresponding probabilities.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;An Example Applying Multiclass Logistic Regression&lt;/h3&gt;
&lt;p&gt;To see an example applying multiclass logistic regression classification, &lt;a href="https://github.com/stlong0521/logistic-classification"&gt;click here&lt;/a&gt; for more information.&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;C. M. Bishop, &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. New York: Springer, 2006.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' &amp;&amp; location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Sun, 28 Feb 2016 00:00:00 -0500</pubDate><guid>tag:stlong0521.github.io,2016-02-28:20160228 - Logistic Regression.html</guid><category>Data Science</category><category>Machine Learning</category></item><item><title>About Tianlong Song</title><link>https://stlong0521.github.io/about.html</link><description>&lt;p&gt;Tianlong Song is currently a PhD student in the Department of Electrical &amp;amp; Computer Engineering at Michigan State University. His interests are primarily focused on data science, machine learning, natural language processing and software engineering.&lt;/p&gt;
&lt;p&gt;His blog keeps the records on how he moved forward little by little in these areas, and he would love to share them with anyone who might be interested. He is happy to exchange ideas in any way, so please do not hesitate to reach him via the email below.&lt;/p&gt;
&lt;p&gt;Tianlong's Email: stlong0521@gmail.com&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tianlong Song</dc:creator><pubDate>Thu, 25 Feb 2016 00:00:00 -0500</pubDate><guid>tag:stlong0521.github.io,2016-02-25:about.html</guid></item></channel></rss>