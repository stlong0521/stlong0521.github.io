<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <title>Tianlong's Blog - Locating and Filling Missing Words in Sentences</title>
    <meta name="description" content="">
    <meta name="author" content="Tianlong Song">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://stlong0521.github.io/theme/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="https://stlong0521.github.io/theme/bootstrap.min.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/bootstrap.min.responsive.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/local.css" rel="stylesheet">
    <link href="https://stlong0521.github.io/theme/pygments.css" rel="stylesheet">

    <!-- So Firefox can bookmark->"abo this site" -->
        <link href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate" title="Tianlong's Blog" type="application/atom+xml">
        <link href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate" title="Tianlong's Blog" type="application/rss+xml">

</head>

<body>

<div class="navbar">
    <div class="navbar-inner">
    <div class="container">

         <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
             <span class="icon-bar"></span>
         </a>

        <a class="brand" href="https://stlong0521.github.io">Tianlong's Blog</a>

        <div class="nav-collapse">
        <ul class="nav">
            <li><a href="/index.html">Home</a></li>
            
        </ul>
        </div>
        
    </div>
    </div>
</div>

<div class="container">
    <div class="content">
    <div class="row">

        <div class="span9">
    <div class='article'>
        <div class="content-title">
            <h1>Locating and Filling Missing Words in Sentences</h1>
Sat 05 Mar 2016

by <a class="url fn" href="https://stlong0521.github.io/author/tianlong-song.html">Tianlong Song</a>
 


 
    Tags <a href="https://stlong0521.github.io/tag/natural-language-processing.html">Natural Language Processing</a>         </div>
	
        <div><p>There has been many occasions that we have incomplete sentences that are needed to completed. One example is that in speech recognition noisy environment can lead to unrecognizable words, but we still hope to recover and understand the complete sentence (e.g., by inference); another example is sentence completion questions that appear in language tests (e.g., SAT, GRE, etc.).</p>
<h3>What are Exactly the Problem?</h3>
<p>Generally, the problem we are aiming to solve is locating and filling any missing words in incomplete sentences. However, this problem seems too ambitious so far, and we direct ourselves to a simplified version of this problem. To simplify the problem, we assume that there is only one missing word in a sentence, and the missing word is neither the first word nor the last word of the sentence. This problem originally comes from <a href="https://www.kaggle.com/c/billion-word-imputation">here</a>.</p>
<h3>Locating the Missing Word</h3>
<p>Two approaches are presented here so as to locate the missing word.</p>
<h4>N-gram Model</h4>
<p>For a given training data set, define <span class="math">\(C(w_1,w_2)\)</span> as the number of occurrences of the bigram pattern <span class="math">\((w_1,w_2)\)</span>, and <span class="math">\(C(w_1,w,w_2)\)</span> the number of occurrences of the trigram pattern <span class="math">\((w_1,w,w_2)\)</span>. Then, the number of occurrences of the pattern, where there is one and only one word between <span class="math">\(w_1\)</span> and <span class="math">\(w_2\)</span>, can be calculated by
</p>
<div class="math">\begin{equation}
D(w_1,w_2)=\sum_{w\in{V}}C(w_1,w,w_2),
\end{equation}</div>
<p>
where <span class="math">\(V\)</span> is the vocabulary.</p>
<p>Consider a particular location, <span class="math">\(l\)</span>, of an incomplete sentence of length <span class="math">\(L\)</span>, and let <span class="math">\(w_l\)</span> be the <span class="math">\(l\)</span>th word in the sentence. <span class="math">\(D(w_{l-1},w_{l})\)</span> would be the number of positive votes from the training data set for missing word at this location, while <span class="math">\(C(w_{l-1},w_{l})\)</span> would be correspondingly the number of negative votes. We define the score indicating there is a missing word at location <span class="math">\(l\)</span> as
</p>
<div class="math">\begin{equation} \label{Eqn:Score}
S_l=\frac{D(w_{l-1},w_{l})^{1+\gamma}}{C(w_{l-1},w_{l})+D(w_{l-1},w_{l})}-\frac{C(w_{l-1},w_{l})^{1+\gamma}}{C(w_{l-1},w_{l})+D(w_{l-1},w_{l})},
\end{equation}</div>
<p>
where <span class="math">\(\gamma\)</span> is a small positive constant. Hence, the missing word location can be identified by
</p>
<div class="math">\begin{equation}
\hat{l}={\arg \, \max}_{1 \leq l \leq L-1} S_l.
\end{equation}</div>
<p>Note that in (\ref{Eqn:Score}), if we set <span class="math">\(\gamma=0\)</span>, the left part would be exactly the percentage of positive votes for missing word at that location, and the right part is the percentage of negative votes. It seems a fairly reasonable score, then <em>why do we still need a positive <span class="math">\(\gamma\)</span></em>? The underlying reason is that intuitively the more number of votes for a particular decision, the more confident we are on that decision. This trend is reflected in a positive <span class="math">\(\gamma\)</span>, which can be viewed as <em>sparse vote penalty</em> and is useful in breaking ties in the missing word location voting. That is, if we have exactly the same ratio of positive votes relative to negative votes for two candidate locations, e.g., 80 positive votes v.s. 20 negative votes for location A, and 8 positive votes v.s. 2 negative votes for location B, we would believe that location A is more likely to be the missing word location compared with location B.</p>
<h4>Word Distance Statistics (WDS)</h4>
<p>In view of the fact that the statistics of the two words immediately adjacent to a given location contribute a lot in deciding whether the location has a word missing, we tentatively guess that all the words within a window centered at that location would more or less contribute some information as well. As a result, we introduce the concept of word distance statistics (WDS).</p>
<p>More specifically, we use <span class="math">\(\widetilde{C}(w_1,w_2,m)\)</span> to denote the number of occurrences of the pattern, where there is exactly <span class="math">\(m\)</span> words between <span class="math">\(w_1\)</span> and <span class="math">\(w_2\)</span>, i.e., the word distance of <span class="math">\(w_1\)</span> and <span class="math">\(w_2\)</span> is <span class="math">\(m\)</span>. For a given location <span class="math">\(l\)</span> in an incomplete sentence and a word window size <span class="math">\(W\)</span>, we are interested in the word distance statistics of each word pair, in which one word <span class="math">\(w_i\)</span> is on the left of the location <span class="math">\(l\)</span>, and the other word <span class="math">\(w_j\)</span> is on the right, as illustrated in Fig. 1.</p>
<figure align="center">
<img src="/figures/20160305/WDS.png" alt="WDS">
<figcaption align="center">Fig. 1: Word distance illustration.</figcaption>
</figure>

<p>Formally, for any <span class="math">\(l-W/2 \leq i \leq l-1\)</span> and <span class="math">\(l \leq j \leq l+W/2-1\)</span>, <span class="math">\(\widetilde{C}(w_i,w_j,j-i)\)</span> would be the number of positive votes for missing word at this location, while <span class="math">\(\widetilde{C}(w_i,w_j,j-i-1)\)</span> is the number of negative votes. Applying the idea in (\ref{Eqn:Score}), for each word pair <span class="math">\((w_i,w_j)\)</span>, we extract its feature as the score indicating there is a missing word at location <span class="math">\(l\)</span>, i.e.,
</p>
<div class="math">\begin{equation} \label{Eqn:ScoreGeneralized}
S_l(i,j)=\frac{\widetilde{C}(w_i,w_j,j-i)^{1+\gamma}}{\widetilde{C}(w_i,w_j,j-i)+\widetilde{C}(w_i,w_j,j-i-1)}-\frac{\widetilde{C}(w_i,w_j,j-i-1)^{1+\gamma}}{\widetilde{C}(w_i,w_j,j-i)+\widetilde{C}(w_i,w_j,j-i-1)}.
\end{equation}</div>
<p>
As a special example, let <span class="math">\(i=l-1\)</span> and <span class="math">\(j=l\)</span>, (\ref{Eqn:ScoreGeneralized}) would be reduced to (\ref{Eqn:Score}).</p>
<p>To find the missing word location, we need to assign different weights to the extracted features, <span class="math">\(S_l(i,j)\)</span>. Then, the missing word location can be determined by
</p>
<div class="math">\begin{equation} \label{Eqn:LocationDetermination}
\hat{l}={\arg \, \max}_{1 \leq l \leq L-1} \sum_{l-\frac{W}{2} \leq i \leq l-1}\sum_{l \leq j \leq l+\frac{W}{2}-1}v(i,j)S_l(i,j),
\end{equation}</div>
<p>
where the weight, <span class="math">\(v(i,j)\)</span>, should be monotonically decreasing with respect to <span class="math">\(|j-i|\)</span>.</p>
<h3>Filling the Missing Word</h3>
<p>To find the most probable word in the given missing word location, we take into account five conditional probabilities, as shown in Table 1, to explore the statistical connection between the candidate words and the surrounding words at the missing word location. Ultimately, the most probable missing word can be determined by
</p>
<div class="math">\begin{equation}
\hat{w}={\arg \, \max}_{w\in{B}} \sum_{1 \leq i \leq 5} v_iP_i,
\end{equation}</div>
<p>
where <span class="math">\(B\)</span> is the candidate word space (detailed <a href="https://github.com/stlong0521/missing-word/blob/master/Project%20Report.pdf">here</a>), and the weight <span class="math">\(v_i\)</span> is used to reflect the importance of each conditional probability in contributing to the final score.</p>
<figure align="center">
<img src="/figures/20160305/CondProb.png" alt="CondProb">
<figcaption align="center">Table 1: Conditional probabilities considered in missing word filling, in which "*" denotes an arbitrary word.</figcaption>
</figure>

<h3>Experimental Results</h3>
<p>The training data contains <span class="math">\(30,301,028\)</span> complete sentences, of which the average sentence length is approximately <span class="math">\(25\)</span>. In the vocabulary with a size of <span class="math">\(2,425,337\)</span>, <span class="math">\(14,216\)</span> words that have occurred in at least <span class="math">\(0.1\%\)</span> of total sentences are labeled as high-frequency words, and the remaining <span class="math">\(58,417,315\)</span> words are labeled as 'UNKA'. To perform the cross validation, in our experiments, the training data is splitted into two part, TRAIN and DEV. The TRAIN set is used to train our models, and the DEV set is applied to test our models.</p>
<h4>Missing Word Location</h4>
<p>Table 2 shows the estimation accuracy of the missing word locations for the two proposed approaches, N-gram and WDS. For comparison, we list the corresponding probabilities by chance as well. Each entry shows the probabilities that the correct location is included in the ranked candidate location list returned by each approach, where the list size varies from <span class="math">\(1\)</span> to <span class="math">\(10\)</span>. The sparse vote penalty coefficient, <span class="math">\(\gamma\)</span>, is set to 0.01. In the WDS approach, we consider a word window size <span class="math">\(W=4\)</span>, i.e., four pairs of words are taken into account.</p>
<table class="table table-striped table-bordered table-hover">
<thead>
<tr>
<th align="center"></th>
<th align="center">Top 1</th>
<th align="center">Top 2</th>
<th align="center">Top 3</th>
<th align="center">Top 5</th>
<th align="center">Top 10</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Chance</td>
<td align="center">4%</td>
<td align="center">8%</td>
<td align="center">12%</td>
<td align="center">20%</td>
<td align="center">40%</td>
</tr>
<tr>
<td align="center">N-gram</td>
<td align="center">51.47%</td>
<td align="center">63.70%</td>
<td align="center">71.00%</td>
<td align="center">80.26%</td>
<td align="center">91.54%</td>
</tr>
<tr>
<td align="center">WDS</td>
<td align="center">52.06%</td>
<td align="center">64.50%</td>
<td align="center">71.76%</td>
<td align="center">80.91%</td>
<td align="center">91.93%</td>
</tr>
</tbody>
</table>
<figcaption align="center">Table 2: Accuracy of missing word location.</figcaption>

<h4>Missing Word Filling</h4>
<p>Table 3 shows the accuracies of filling the missing word given the location. Each row of the second column shows the probability that the correct word is included in the ranked candidate words list returned by the proposed approach.</p>
<table class="table table-striped table-bordered table-hover">
<thead>
<tr>
<th align="center"></th>
<th align="center">Top 1</th>
<th align="center">Top 2</th>
<th align="center">Top 3</th>
<th align="center">Top 5</th>
<th align="center">Top 10</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">Accuracy</td>
<td align="center">32.15%</td>
<td align="center">41.49%</td>
<td align="center">46.23%</td>
<td align="center">52.02%</td>
<td align="center">59.15%</td>
</tr>
</tbody>
</table>
<figcaption align="center">Table 3: Accuracy of missing word filling.</figcaption>

<h3>Acknowledgement</h3>
<p>I did this project with my partner, <a href="https://zhwa.github.io/">Zhe Wang</a>. To see the codes and/or report, <a href="https://github.com/stlong0521/missing-word">click here</a> for more information.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' }, Macros: {} }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
	
        <hr>

        <h2>Comments</h2>
<div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'tianlongsong'; 
    var disqus_title = 'Locating and Filling Missing Words in Sentences';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    </div>
        </div>
        
        <div class="span3">

            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Site
                </li>
            
                <li><a href="https://stlong0521.github.io/archives.html">Archives</a>
                <li><a href="https://stlong0521.github.io/tags.html">Tags</a>



                <li><a href="https://stlong0521.github.io/feeds/all.atom.xml" rel="alternate">Atom feed</a></li>
                <li><a href="https://stlong0521.github.io/feeds/all.rss.xml" rel="alternate">RSS feed</a></li>

            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Categories
                </li>
                
                <li><a href="https://stlong0521.github.io/category/big-data.html">Big Data</a></li>
                <li><a href="https://stlong0521.github.io/category/machine-learning.html">Machine Learning</a></li>
                <li><a href="https://stlong0521.github.io/category/natural-language-processing.html">Natural Language Processing</a></li>
                   
            </ul>
            </div>


            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Links
                </li>
            
                <li><a href="/webpage/index.html">About Tianlong</a></li>
            </ul>
            </div>


            <div class="social">
            <div class="well" style="padding: 8px 0; background-color: #FBFBFB;">
            <ul class="nav nav-list">
                <li class="nav-header"> 
                Social
                </li>
           
                <li><a href="https://www.linkedin.com/in/tianlongsong">LinkedIn</a></li>
                <li><a href="https://github.com/stlong0521">GitHub</a></li>
                <li><a href="https://www.facebook.com/tianlong.song.3">Facebook</a></li>
                <li><a href="https://twitter.com/stlong0521">Twitter</a></li>
            </ul>
            </div>
            </div>

        </div>  
    </div>     </div> 
<footer>
<br />
<p><a href="https://stlong0521.github.io">Tianlong's Blog</a> &copy; Tianlong Song</p>
</footer>

</div> <!-- /container -->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
<script src="https://stlong0521.github.io/theme/bootstrap-collapse.js"></script>
<script>var _gaq=[['_setAccount','UA-74544804-1'],['_trackPageview']];(function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];g.src='//www.google-analytics.com/ga.js';s.parentNode.insertBefore(g,s)}(document,'script'))</script>
 
</body>
</html>